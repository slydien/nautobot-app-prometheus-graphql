{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Nautobot App GraphQL Observability","text":"<p>    A GraphQL observability app for Nautobot \u2014 Prometheus metrics and structured query logging. </p>"},{"location":"index.html#overview","title":"Overview","text":"<p>A Nautobot app that provides comprehensive observability for the GraphQL API. It includes two Graphene middlewares that collect Prometheus metrics and emit structured query logs \u2014 without modifying Nautobot's core code.</p>"},{"location":"index.html#features","title":"Features","text":"<p>Prometheus Metrics (<code>PrometheusMiddleware</code>):</p> <ul> <li>Request metrics: Count and measure the duration of all GraphQL queries and mutations.</li> <li>Error tracking: Count errors by operation and exception type.</li> <li>Query depth &amp; complexity: Histogram metrics for query nesting depth and total field count.</li> <li>Per-user tracking: Count requests per authenticated user for auditing and capacity planning.</li> <li>Per-field resolution: Optionally measure individual field resolver durations for debugging.</li> <li>All metrics appear at Nautobot's default <code>/metrics/</code> endpoint \u2014 no extra endpoint needed.</li> </ul> <p>Query Logging (<code>GraphQLQueryLoggingMiddleware</code>):</p> <ul> <li>Structured log entries: Operation type, name, user, duration, and status for every query.</li> <li>Optional query body and variables: Include the full query text and variables in log entries.</li> <li>Standard Python logging: Route logs to any backend (file, syslog, ELK, etc.) via Django's <code>LOGGING</code> configuration.</li> </ul> <p>General:</p> <ul> <li>Zero configuration: Automatically patches Nautobot's <code>GraphQLDRFAPIView</code> to load the middlewares \u2014 no manual <code>GRAPHENE[\"MIDDLEWARE\"]</code> setup needed.</li> </ul>"},{"location":"index.html#quick-install","title":"Quick Install","text":"<pre><code>pip install nautobot-graphql-observability\n</code></pre> <pre><code># nautobot_config.py\nPLUGINS = [\"nautobot_graphql_observability\"]\n</code></pre>"},{"location":"index.html#documentation","title":"Documentation","text":"<p>Full documentation is bundled with the app and available in the <code>docs</code> folder of this repository:</p> <ul> <li>User Guide (<code>docs/user/</code>) - Overview, Using the App, Getting Started.</li> <li>Administrator Guide (<code>docs/admin/</code>) - How to Install, Configure, Upgrade, or Uninstall the App.</li> <li>Developer Guide (<code>docs/dev/</code>) - Extending the App, Code Reference, Contribution Guide.</li> <li>Release Notes (<code>docs/admin/release_notes/</code>).</li> <li>FAQ (<code>docs/user/faq.md</code>).</li> </ul>"},{"location":"index.html#contributing-to-the-documentation","title":"Contributing to the Documentation","text":"<p>You can find all the Markdown source for the App documentation under the <code>docs</code> folder in this repository. For simple edits, a Markdown capable editor is sufficient: clone the repository and edit away.</p> <p>If you need to view the fully-generated documentation site, you can build it with MkDocs. A container hosting the documentation can be started using the <code>invoke</code> commands on http://localhost:8001. Using this container, as your changes to the documentation are saved, they will be automatically rebuilt and any pages currently being viewed will be reloaded in your browser.</p> <p>Any PRs with fixes or improvements are very welcome!</p>"},{"location":"index.html#questions","title":"Questions","text":"<p>For any questions or comments, please check the FAQ first. Feel free to open an issue on GitHub.</p>"},{"location":"admin/compatibility_matrix.html","title":"Compatibility Matrix","text":"<p>Changes to the support of upstream Nautobot versions will be announced via the release notes for each app version. Users are advised to upgrade to the latest compatible version of this app for their Nautobot release.</p> Nautobot App Prometheus GraphQL Version Nautobot First Support Version Nautobot Last Support Version Python Versions 1.0.x 3.0.0 3.x 3.10, 3.11, 3.12, 3.13"},{"location":"admin/install.html","title":"Installing the App in Nautobot","text":"<p>Here you will find detailed instructions on how to install and configure the App within your Nautobot environment.</p>"},{"location":"admin/install.html#prerequisites","title":"Prerequisites","text":"<ul> <li>The app is compatible with Nautobot 3.0.0 and higher.</li> <li>Databases supported: PostgreSQL, MySQL</li> </ul> <p>Note</p> <p>Please check the dedicated page for a full compatibility matrix and the deprecation policy.</p>"},{"location":"admin/install.html#install-guide","title":"Install Guide","text":"<p>Note</p> <p>Apps can be installed from the Python Package Index or locally. See the Nautobot documentation for more details. The pip package name for this app is <code>nautobot-graphql-observability</code>.</p> <p>The app is available as a Python package via PyPI and can be installed with <code>pip</code>:</p> <pre><code>pip install nautobot-graphql-observability\n</code></pre> <p>To ensure the app is automatically re-installed during future upgrades, create a file named <code>local_requirements.txt</code> (if not already existing) in the Nautobot root directory (alongside <code>requirements.txt</code>) and list the <code>nautobot-graphql-observability</code> package:</p> <pre><code>echo nautobot-graphql-observability &gt;&gt; local_requirements.txt\n</code></pre> <p>Once installed, the app needs to be enabled in your Nautobot configuration. The following block of code below shows the additional configuration required to be added to your <code>nautobot_config.py</code> file:</p> <ul> <li>Append <code>\"nautobot_graphql_observability\"</code> to the <code>PLUGINS</code> list.</li> <li>Optionally append the <code>\"nautobot_graphql_observability\"</code> dictionary to the <code>PLUGINS_CONFIG</code> dictionary to override any defaults.</li> </ul> <pre><code># In your nautobot_config.py\nPLUGINS = [\"nautobot_graphql_observability\"]\n\nPLUGINS_CONFIG = {\n    \"nautobot_graphql_observability\": {\n        # Prometheus metrics settings\n        \"graphql_metrics_enabled\": True,\n        \"track_query_depth\": True,\n        \"track_query_complexity\": True,\n        \"track_field_resolution\": False,\n        \"track_per_user\": True,\n        # Query logging settings\n        \"query_logging_enabled\": False,\n        \"log_query_body\": False,\n        \"log_query_variables\": False,\n    }\n}\n</code></pre> <p>No GRAPHENE middleware configuration needed</p> <p>Unlike typical Graphene middleware, you do not need to manually configure <code>GRAPHENE[\"MIDDLEWARE\"]</code> in your settings. The app's <code>ready()</code> method automatically monkey-patches Nautobot's <code>GraphQLDRFAPIView.init_graphql()</code> to load middleware from <code>GRAPHENE[\"MIDDLEWARE\"]</code>. This works around a bug in Nautobot 3.x where the view does not load middleware when <code>self.middleware</code> is <code>None</code>.</p> <p>Once the Nautobot configuration is updated, run the Post Upgrade command (<code>nautobot-server post_upgrade</code>) to run migrations and clear any cache:</p> <pre><code>nautobot-server post_upgrade\n</code></pre> <p>Then restart (if necessary) the Nautobot services which may include:</p> <ul> <li>Nautobot</li> <li>Nautobot Workers</li> <li>Nautobot Scheduler</li> </ul> <pre><code>sudo systemctl restart nautobot nautobot-worker nautobot-scheduler\n</code></pre>"},{"location":"admin/install.html#app-configuration","title":"App Configuration","text":"<p>The app behavior can be controlled with the following list of settings:</p>"},{"location":"admin/install.html#prometheus-metrics-settings","title":"Prometheus Metrics Settings","text":"Key Type Default Description <code>graphql_metrics_enabled</code> <code>bool</code> <code>True</code> Enable or disable all metrics collection. When <code>False</code>, the Prometheus middleware is a no-op. <code>track_query_depth</code> <code>bool</code> <code>True</code> Record a histogram of GraphQL query nesting depth. <code>track_query_complexity</code> <code>bool</code> <code>True</code> Record a histogram of GraphQL query complexity (total field count). <code>track_field_resolution</code> <code>bool</code> <code>False</code> Record per-field resolver duration. Warning: enabling this adds significant overhead for queries with many fields. <code>track_per_user</code> <code>bool</code> <code>True</code> Record a per-user request counter using the authenticated username."},{"location":"admin/install.html#query-logging-settings","title":"Query Logging Settings","text":"Key Type Default Description <code>query_logging_enabled</code> <code>bool</code> <code>False</code> Enable or disable GraphQL query logging. When <code>False</code>, the logging middleware is a no-op. <code>log_query_body</code> <code>bool</code> <code>False</code> Include the full GraphQL query text in log entries. <code>log_query_variables</code> <code>bool</code> <code>False</code> Include the GraphQL query variables in log entries. Warning: may log sensitive data."},{"location":"admin/install.html#multi-process-deployments","title":"Multi-Process Deployments","text":"<p>If you run Nautobot with multiple worker processes (e.g. via Gunicorn), you must set the <code>PROMETHEUS_MULTIPROC_DIR</code> environment variable to a writable directory so that <code>prometheus_client</code> can aggregate metrics across processes:</p> <pre><code>export PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus_multiproc\nmkdir -p \"$PROMETHEUS_MULTIPROC_DIR\"\n</code></pre> <p>Nautobot's default <code>/metrics/</code> endpoint will automatically aggregate metrics from all worker processes when this variable is set.</p>"},{"location":"admin/uninstall.html","title":"Uninstall the App from Nautobot","text":"<p>Here you will find any steps necessary to cleanly remove the App from your Nautobot environment.</p>"},{"location":"admin/uninstall.html#remove-app-configuration","title":"Remove App Configuration","text":"<p>This app has no database models, so there are no migrations to roll back. Simply remove the configuration from <code>nautobot_config.py</code>:</p> <ol> <li>Remove <code>\"nautobot_graphql_observability\"</code> from the <code>PLUGINS</code> list.</li> <li>Remove the <code>\"nautobot_graphql_observability\"</code> entry from <code>PLUGINS_CONFIG</code> (if present).</li> </ol>"},{"location":"admin/uninstall.html#uninstall-the-package","title":"Uninstall the Package","text":"<pre><code>pip uninstall nautobot-graphql-observability\n</code></pre>"},{"location":"admin/uninstall.html#restart-services","title":"Restart Services","text":"<p>Restart Nautobot services to apply the changes:</p> <pre><code>sudo systemctl restart nautobot nautobot-worker nautobot-scheduler\n</code></pre>"},{"location":"admin/upgrade.html","title":"Upgrading the App","text":"<p>Here you will find any steps necessary to upgrade the App in your Nautobot environment.</p>"},{"location":"admin/upgrade.html#upgrade-guide","title":"Upgrade Guide","text":"<p>This app has no database models and therefore requires no database migrations. To upgrade:</p> <ol> <li> <p>Update the package:</p> <pre><code>pip install --upgrade nautobot-graphql-observability\n</code></pre> </li> <li> <p>Run <code>nautobot-server post_upgrade</code> to clear caches and collect static files:</p> <pre><code>nautobot-server post_upgrade\n</code></pre> </li> <li> <p>Restart Nautobot services:</p> <pre><code>sudo systemctl restart nautobot nautobot-worker nautobot-scheduler\n</code></pre> </li> <li> <p>Review the release notes for any new configuration options added in the new version.</p> </li> </ol>"},{"location":"admin/release_notes/index.html","title":"Release Notes","text":"<p>All the published release notes can be found via the navigation menu. All patch releases are included in the same minor release (e.g. <code>v1.2</code>) document.</p>"},{"location":"admin/release_notes/version_1.0.html","title":"v1.0 Release Notes","text":"<p>This document describes all new features and changes in the release <code>1.0</code>. The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"admin/release_notes/version_1.0.html#release-overview","title":"Release Overview","text":"<ul> <li>Initial release of the Prometheus GraphQL Metrics middleware for Nautobot.</li> <li>Provides 7 Prometheus metrics covering request counts, durations, errors, query depth, query complexity, per-field resolution, and per-user tracking.</li> <li>Automatic monkey-patching of Nautobot's <code>GraphQLDRFAPIView</code> to load Graphene middleware from settings.</li> <li>Metrics are automatically available at Nautobot's default <code>/metrics/</code> endpoint.</li> </ul>"},{"location":"admin/release_notes/version_1.0.html#v100-2025","title":"[v1.0.0] - 2025","text":""},{"location":"admin/release_notes/version_1.0.html#added","title":"Added","text":"<ul> <li>Graphene middleware (<code>PrometheusMiddleware</code>) that instruments GraphQL queries with Prometheus metrics.</li> <li>Basic metrics: <code>graphql_requests_total</code>, <code>graphql_request_duration_seconds</code>, <code>graphql_errors_total</code>.</li> <li>Advanced metrics: <code>graphql_query_depth</code>, <code>graphql_query_complexity</code>, <code>graphql_field_resolution_duration_seconds</code>, <code>graphql_requests_by_user_total</code>.</li> <li>Configurable settings to enable/disable individual metric categories.</li> <li>Metrics registered in the default Prometheus registry and available at Nautobot's <code>/metrics/</code> endpoint.</li> <li>Automatic patching of <code>GraphQLDRFAPIView.init_graphql()</code> to work around Nautobot 3.x middleware loading bug.</li> <li>Multi-process support via <code>PROMETHEUS_MULTIPROC_DIR</code>.</li> <li>Grafana dashboard templates for GraphQL performance monitoring.</li> </ul>"},{"location":"admin/release_notes/version_2.0.html","title":"v2.0 Release Notes","text":"<p>This document describes all new features and changes in the release <code>2.0</code>. The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"admin/release_notes/version_2.0.html#release-overview","title":"Release Overview","text":"<ul> <li>Package renamed from <code>nautobot-app-graphql-observability</code> to <code>nautobot-graphql-observability</code>.</li> <li>Refactored to use official Nautobot extension points instead of monkey patches where possible.</li> <li>Added Django HTTP middleware for accurate request timing on both GraphQL endpoints.</li> </ul>"},{"location":"admin/release_notes/version_2.0.html#v200-2026","title":"[v2.0.0] - 2026","text":""},{"location":"admin/release_notes/version_2.0.html#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Package renamed: distribution name changed from <code>nautobot-app-graphql-observability</code> to <code>nautobot-graphql-observability</code>.</li> <li>Module renamed: Python module changed from <code>nautobot_app_graphql_observability</code> to <code>nautobot_graphql_observability</code>.</li> <li>Update all references in <code>nautobot_config.py</code> (<code>PLUGINS</code>, <code>PLUGINS_CONFIG</code>, <code>GRAPHENE[\"MIDDLEWARE\"]</code>).</li> </ul>"},{"location":"admin/release_notes/version_2.0.html#added","title":"Added","text":"<ul> <li><code>GraphQLObservabilityDjangoMiddleware</code> \u2014 Django HTTP middleware registered via official <code>NautobotAppConfig.middleware</code> mechanism.</li> <li>Handles request timing and query logging for both <code>/api/graphql/</code> and <code>/graphql/</code> endpoints.</li> <li><code>stash_meta_on_request</code> shared utility for DRF/WSGI request metadata stashing.</li> </ul>"},{"location":"admin/release_notes/version_2.0.html#changed","title":"Changed","text":"<ul> <li>Replaced monkey patches on <code>GraphQLDRFAPIView.post()</code> and <code>CustomGraphQLView.dispatch()</code> with official Django middleware.</li> <li>Only one monkey patch remains: <code>_patch_init_graphql()</code> to work around Nautobot's middleware loading limitation (no official extension point available).</li> </ul>"},{"location":"admin/release_notes/version_2.0.html#migration-guide","title":"Migration Guide","text":"<pre><code># Before (nautobot_config.py)\nPLUGINS = [\"nautobot_app_graphql_observability\"]\nPLUGINS_CONFIG = {\"nautobot_app_graphql_observability\": { ... }}\n\n# After\nPLUGINS = [\"nautobot_graphql_observability\"]\nPLUGINS_CONFIG = {\"nautobot_graphql_observability\": { ... }}\n</code></pre> <pre><code>pip uninstall nautobot-app-graphql-observability\npip install nautobot-graphql-observability\n</code></pre>"},{"location":"admin/release_notes/version_2.1.html","title":"v2.1 Release Notes","text":"<p>This document describes all new features and changes in the release. The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"admin/release_notes/version_2.1.html#release-overview","title":"Release Overview","text":"<ul> <li>Structured JSON logging via Nautobot's built-in <code>setup_structlog_logging()</code>: all loggers now emit JSON in production with each query log field (<code>operation_type</code>, <code>operation_name</code>, <code>user</code>, <code>duration_ms</code>, <code>status</code>, etc.) as a top-level key instead of a packed string.</li> </ul>"},{"location":"admin/release_notes/version_2.1.html#v210-2026-02-19","title":"v2.1.0 (2026-02-19)","text":""},{"location":"admin/release_notes/version_2.1.html#added","title":"Added","text":"<ul> <li>Added structlog JSON logging support: <code>setup_structlog_logging()</code> integration in <code>nautobot_config.py</code> routes all loggers (<code>django</code>, <code>nautobot</code>, <code>django.request</code>, and <code>nautobot_graphql_observability.graphql_query_log</code>) through a single structlog handler. With <code>plain_format=False</code> (production default), all log output is emitted as structured JSON. The query log middleware now emits operation metadata as discrete <code>extra</code> fields (<code>operation_type</code>, <code>operation_name</code>, <code>user</code>, <code>duration_ms</code>, <code>status</code>, etc.) rather than a flat key=value string, so each field appears as a top-level JSON key in log aggregators. <code>structlog.stdlib.ExtraAdder()</code> is appended to the formatter's <code>foreign_pre_chain</code> to promote these fields automatically.</li> </ul>"},{"location":"dev/arch_decision.html","title":"Architecture Decision Records","text":""},{"location":"dev/arch_decision.html#adr-1-graphene-middleware-vs-django-middleware","title":"ADR-1: Graphene Middleware vs Django Middleware","text":"<p>Decision: Use a Graphene middleware (not a Django HTTP middleware) for metrics instrumentation.</p> <p>Context: Django middleware operates at the HTTP request/response level and cannot inspect GraphQL-specific details like operation names, types, query depth, or individual field resolution. Graphene middleware is invoked for each field resolution and has access to the <code>GraphQLResolveInfo</code> object with full query metadata.</p> <p>Consequence: The middleware can label metrics with <code>operation_type</code>, <code>operation_name</code>, <code>type_name</code>, and <code>field_name</code> labels that would be unavailable at the HTTP layer.</p>"},{"location":"dev/arch_decision.html#adr-2-monkey-patching-graphqldrfapiviewinit_graphql","title":"ADR-2: Monkey-Patching GraphQLDRFAPIView.init_graphql","text":"<p>Decision: Monkey-patch <code>GraphQLDRFAPIView.init_graphql()</code> in the app's <code>AppConfig.ready()</code> method.</p> <p>Context: Nautobot 3.x's <code>GraphQLDRFAPIView.init_graphql()</code> does not load middleware from the <code>GRAPHENE[\"MIDDLEWARE\"]</code> setting when <code>self.middleware</code> is <code>None</code>. This is a bug in Nautobot's implementation. Without patching, configured Graphene middleware is silently ignored.</p> <p>Alternatives considered:</p> <ul> <li>Forking Nautobot: Too heavy and maintenance-intensive.</li> <li>Overriding the URL route: Would require duplicating the view class and URL configuration.</li> <li>Subclassing the view: <code>GraphQLDRFAPIView</code> is referenced directly in Nautobot's URL configuration, so a subclass would not be used.</li> </ul> <p>Consequence: The patch is minimal (wraps the original method, only acts when <code>self.middleware is None</code>) and is applied once at startup. It introduces a coupling to Nautobot's internal API that may need updating if Nautobot fixes the bug upstream.</p>"},{"location":"dev/arch_decision.html#adr-3-timemonotonic-for-duration-measurement","title":"ADR-3: time.monotonic() for Duration Measurement","text":"<p>Decision: Use <code>time.monotonic()</code> instead of <code>time.time()</code> for duration measurements.</p> <p>Context: <code>time.monotonic()</code> is immune to system clock adjustments (NTP corrections, manual changes) and provides consistent interval measurement. <code>time.time()</code> can produce negative durations if the clock is adjusted backward.</p> <p>Consequence: Duration metrics are always non-negative and accurate regardless of clock adjustments.</p>"},{"location":"dev/arch_decision.html#adr-4-metric-label-design","title":"ADR-4: Metric Label Design","text":"<p>Decision: Use a fixed set of low-cardinality labels. Operation names are included as labels but field-level tracking is opt-in.</p> <p>Context: Prometheus best practices recommend keeping label cardinality low. Operation names can have moderate cardinality (typically tens to low hundreds of unique names in a Nautobot deployment). Field-level labels (<code>type_name</code>, <code>field_name</code>) can have higher cardinality and are gated behind the <code>track_field_resolution</code> setting (disabled by default).</p> <p>Consequence: Basic metrics are safe for production use. Per-field metrics should only be enabled for short-term debugging to avoid cardinality explosion in Prometheus.</p>"},{"location":"dev/arch_decision.html#adr-5-root-only-instrumentation-for-basic-metrics","title":"ADR-5: Root-Only Instrumentation for Basic Metrics","text":"<p>Decision: Only record basic metrics (request count, duration, errors) at the root resolver level (<code>root is None</code>).</p> <p>Context: Graphene middleware is called for every field resolution in a query. Recording metrics at every level would multiply the overhead by the number of fields and produce misleading counts (one query would generate hundreds of metric increments).</p> <p>Consequence: Basic metrics accurately represent one increment per GraphQL operation. Per-field instrumentation is a separate opt-in feature.</p>"},{"location":"dev/contributing.html","title":"Contributing to the App","text":"<p>The project is packaged with a light development environment based on <code>docker-compose</code> to help with the local development of the project and to run tests.</p> <p>The project is following Network to Code software development guidelines and is leveraging the following:</p> <ul> <li>Python linting and formatting: <code>pylint</code> and <code>ruff</code>.</li> <li>YAML linting is done with <code>yamllint</code>.</li> <li>Django unit test to ensure the app is working properly.</li> <li>Django Template linting: <code>djlint</code></li> <li>Django Template formatting: <code>djhtml</code></li> </ul> <p>Documentation is built using mkdocs. The Docker based development environment automatically starts a container hosting a live version of the documentation website on http://localhost:8001 that auto-refreshes when you make any changes to your local files.</p>"},{"location":"dev/contributing.html#creating-changelog-fragments","title":"Creating Changelog Fragments","text":"<p>All pull requests to <code>next</code> or <code>develop</code> must include a changelog fragment file in the <code>./changes</code> directory. To create a fragment, use your GitHub issue number and fragment type as the filename. For example, <code>2362.added</code>. Valid fragment types are <code>added</code>, <code>changed</code>, <code>deprecated</code>, <code>fixed</code>, <code>removed</code>, and <code>security</code>. The change summary is added to the file in plain text. Change summaries should be complete sentences, starting with a capital letter and ending with a period, and be in past tense. Each line of the change fragment will generate a single change entry in the release notes. Use multiple lines in the same file if your change needs to generate multiple release notes in the same category. If the change needs to create multiple entries in separate categories, create multiple files.</p> <p>Example</p> <p>Wrong changes/1234.fixed<pre><code>fix critical bug in documentation\n</code></pre></p> <p>Right changes/1234.fixed<pre><code>Fixed critical bug in documentation.\n</code></pre></p> <p>Multiple Entry Example</p> <p>This will generate 2 entries in the <code>fixed</code> category and one entry in the <code>changed</code> category.</p> changes/1234.fixed<pre><code>Fixed critical bug in documentation.\nFixed release notes generation.\n</code></pre> changes/1234.changed<pre><code>Changed release notes generation.\n</code></pre>"},{"location":"dev/contributing.html#branching-policy","title":"Branching Policy","text":"<p>The branching policy includes the following tenets:</p> <ul> <li>The <code>develop</code> branch is the branch of the next major and minor paired version planned.</li> <li>PRs intended to add new features should be sourced from the <code>develop</code> branch.</li> <li>PRs intended to fix issues in the Nautobot LTM compatible release should be sourced from the latest <code>ltm-&lt;major.minor&gt;</code> branch instead of <code>develop</code>.</li> </ul> <p>Nautobot App GraphQL Observability will observe semantic versioning, as of 1.0. This may result in a quick turnaround in minor versions to keep pace with an ever-growing feature set.</p>"},{"location":"dev/contributing.html#backporting-to-older-releases","title":"Backporting to Older Releases","text":"<p>If you are backporting any fixes to a prior major or minor version of this app, please open an issue, comment on an existing issue, or post in the Network to Code Slack (channel <code>#nautobot</code>).</p> <p>We will create a <code>release-X.Y</code> branch for you to open your PR against and cut a new release once the PR is successfully merged.</p>"},{"location":"dev/contributing.html#release-policy","title":"Release Policy","text":"<p>Nautobot App GraphQL Observability has currently no intended scheduled release schedule, and will release new features in minor versions.</p> <p>The steps taken by maintainers when creating a new release are documented in the release checklist.</p>"},{"location":"dev/dev_environment.html","title":"Building Your Development Environment","text":""},{"location":"dev/dev_environment.html#quickstart-guide","title":"Quickstart Guide","text":"<p>The development environment can be used in two ways:</p> <ol> <li>(Recommended) All services, including Nautobot, are spun up using Docker containers and a volume mount so you can develop locally.</li> <li>With a local Poetry environment if you wish to develop outside of Docker, with the caveat of using external services provided by Docker for the database (PostgreSQL by default, MySQL optionally) and Redis services.</li> </ol> <p>This is a quick reference guide if you're already familiar with the development environment provided, which you can read more about later in this document.</p>"},{"location":"dev/dev_environment.html#invoke","title":"Invoke","text":"<p>The Invoke library is used to provide some helper commands based on the environment. There are a few configuration parameters which can be passed to Invoke to override the default configuration:</p> <ul> <li><code>nautobot_ver</code>: the version of Nautobot to use as a base for any built docker containers (default: 3.0.0)</li> <li><code>project_name</code>: the default docker compose project name (default: <code>nautobot-graphql-observability</code>)</li> <li><code>python_ver</code>: the version of Python to use as a base for any built docker containers (default: 3.12)</li> <li><code>local</code>: a boolean flag indicating if invoke tasks should be run on the host or inside the docker containers (default: False, commands will be run in docker containers)</li> <li><code>compose_dir</code>: the full path to a directory containing the project compose files</li> <li><code>compose_files</code>: a list of compose files applied in order (see Multiple Compose files for more information)</li> </ul> <p>Using Invoke these configuration options can be overridden using several methods. Perhaps the simplest is setting an environment variable <code>INVOKE_NAUTOBOT_GRAPHQL_OBSERVABILITY_VARIABLE_NAME</code> where <code>VARIABLE_NAME</code> is the variable you are trying to override. The only exception is <code>compose_files</code>, because it is a list it must be overridden in a YAML file. There is an example <code>invoke.yml</code> (<code>invoke.example.yml</code>) in this directory which can be used as a starting point.</p>"},{"location":"dev/dev_environment.html#docker-development-environment","title":"Docker Development Environment","text":"<p>Tip</p> <p>This is the recommended option for development.</p> <p>This project is managed by Python Poetry and has a few requirements to setup your development environment:</p> <ol> <li>Install Poetry, see the Poetry documentation for your operating system.</li> <li>Install Docker, see the Docker documentation for your operating system.</li> <li>Install Docker-compose, see the Docker-compose documentation for your operation system.</li> </ol> <p>Once you have Poetry and Docker installed you can run the following commands (in the root of the repository) to install all other development dependencies in an isolated Python virtual environment:</p> <pre><code>poetry self add poetry-plugin-shell\npoetry shell\npoetry install\ninvoke build\ninvoke start\n</code></pre> <p>The Nautobot server can now be accessed at http://localhost:8080 and the live documentation at http://localhost:8001.</p> <p>To either stop or destroy the development environment use the following options.</p> <ul> <li>invoke stop - Stop the containers, but keep all underlying systems intact</li> <li>invoke destroy - Stop and remove all containers, volumes, etc. (This results in data loss due to the volume being deleted)</li> </ul>"},{"location":"dev/dev_environment.html#local-poetry-development-environment","title":"Local Poetry Development Environment","text":"<ul> <li>Create an <code>invoke.yml</code> file with the following contents at the root of the repo and edit as necessary</li> </ul> <pre><code>---\nnautobot_graphql_observability:\n  local: true\n</code></pre> <p>Run the following commands:</p> <pre><code>poetry self add poetry-plugin-shell\npoetry shell\npoetry install --extras nautobot\nexport $(cat development/development.env | xargs)\nexport $(cat development/creds.env | xargs)\ninvoke start &amp;&amp; sleep 5\nnautobot-server migrate\n</code></pre> <p>Note</p> <p>If you want to develop on the latest develop branch of Nautobot, run the following command: <code>poetry add --optional git+https://github.com/nautobot/nautobot@develop</code>. After the <code>@</code> symbol must match either a branch or a tag.</p> <p>You can now run <code>nautobot-server</code> commands as you would from the Nautobot documentation for example to start the development server:</p> <pre><code>nautobot-server runserver 0.0.0.0:8080 --insecure\n</code></pre> <p>Nautobot server can now be accessed at http://localhost:8080.</p> <p>It is typically recommended to launch the Nautobot runserver command in a separate shell so you can keep developing and manage the webserver separately.</p>"},{"location":"dev/dev_environment.html#updating-the-documentation","title":"Updating the Documentation","text":"<p>Documentation dependencies are pinned to exact versions to ensure consistent results. For the development environment, they are defined in the <code>pyproject.toml</code> file.</p>"},{"location":"dev/dev_environment.html#cli-helper-commands","title":"CLI Helper Commands","text":"<p>The project features a CLI helper based on Invoke to help setup the development environment. The commands are listed below in 3 categories:</p> <ul> <li><code>dev environment</code></li> <li><code>utility</code></li> <li><code>testing</code></li> </ul> <p>Each command can be executed with <code>invoke &lt;command&gt;</code>. All commands support the arguments <code>--nautobot-ver</code> and <code>--python-ver</code> if you want to manually define the version of Python and Nautobot to use. Each command also has its own help <code>invoke &lt;command&gt; --help</code></p>"},{"location":"dev/dev_environment.html#local-development-environment","title":"Local Development Environment","text":"<pre><code>  build            Build all docker images.\n  debug            Start Nautobot and its dependencies in debug mode.\n  destroy          Destroy all containers and volumes.\n  restart          Restart Nautobot and its dependencies in detached mode.\n  start            Start Nautobot and its dependencies in detached mode.\n  stop             Stop Nautobot and its dependencies.\n</code></pre>"},{"location":"dev/dev_environment.html#utility","title":"Utility","text":"<pre><code>  cli              Launch a bash shell inside the running Nautobot container.\n  create-user      Create a new user in django (default: admin), will prompt for password.\n  makemigrations   Run Make Migration in Django.\n  nbshell          Launch a nbshell session.\n</code></pre>"},{"location":"dev/dev_environment.html#testing","title":"Testing","text":"<pre><code>  ruff             Run ruff to perform code formatting and/or linting.\n  pylint           Run pylint code analysis.\n  markdownlint     Run pymarkdown linting.\n  tests            Run all tests for this app.\n  unittest         Run Django unit tests for the app.\n  djlint           Run djlint to perform django template linting.\n  djhtml           Run djhtml to perform django template formatting.\n</code></pre>"},{"location":"dev/dev_environment.html#project-overview","title":"Project Overview","text":"<p>This project provides the ability to develop and manage the Nautobot server locally (with supporting services being Dockerized) or by using only Docker containers to manage Nautobot. The main difference between the two environments is the ability to debug and use pdb when developing locally. Debugging with pdb within the Docker container is more complicated, but can still be accomplished by either entering into the container (via <code>docker exec</code>) or attaching your IDE to the container and running the Nautobot service manually within the container.</p> <p>The upside to having the Nautobot service handled by Docker rather than locally is that you do not have to manage the Nautobot server. The Docker logs provide the majority of the information you will need to help troubleshoot, while getting started quickly and not requiring you to perform several manual steps and remembering to have the Nautobot server running in a separate terminal while you develop.</p> <p>Note</p> <p>The local environment still uses Docker containers for the supporting services (Postgres, Redis, and RQ Worker), but the Nautobot server is handled locally by you, the developer.</p> <p>Follow the directions below for the specific development environment that you choose.</p>"},{"location":"dev/dev_environment.html#poetry","title":"Poetry","text":"<p>Poetry is used in lieu of the \"virtualenv\" commands and is leveraged in both environments. The virtual environment will provide all of the Python packages required to manage the development environment such as Invoke. See the Local Development Environment section to see how to install Nautobot if you're going to be developing locally (i.e. not using the Docker container).</p> <p>The <code>pyproject.toml</code> file outlines all of the relevant dependencies for the project:</p> <ul> <li><code>tool.poetry.dependencies</code> - the main list of dependencies.</li> <li><code>tool.poetry.group.dev.dependencies</code> - development dependencies, to facilitate linting, testing, and documentation building.</li> </ul> <p>The <code>poetry shell</code> command is used to create and enable a virtual environment managed by Poetry, so all commands ran going forward are executed within the virtual environment. This is similar to running the <code>source venv/bin/activate</code> command with virtualenvs. To install project dependencies in the virtual environment, you should run <code>poetry install</code> - this will install both project and development dependencies.</p> <p>For more details about Poetry and its commands please check out its online documentation.</p> <p>In Poetry version 2, the shell command was moved out of the main Poetry project and into a plugin. For more details about the Poetry shell plugin, refer to its GitHub repository.</p>"},{"location":"dev/dev_environment.html#full-docker-development-environment","title":"Full Docker Development Environment","text":"<p>This project is set up with a number of Invoke tasks consumed as simple CLI commands to get developing fast. You'll use a few <code>invoke</code> commands to get your environment up and running.</p>"},{"location":"dev/dev_environment.html#copy-the-credentials-file-for-nautobot","title":"Copy the credentials file for Nautobot","text":"<p>First, you may create/overwrite the <code>development/creds.env</code> file - it stores a bunch of private information such as passwords and tokens for your local Nautobot install. You can make a copy of the <code>development/creds.example.env</code> and modify it to suit you.</p> <pre><code>cp development/creds.example.env development/creds.env\n</code></pre>"},{"location":"dev/dev_environment.html#invoke-building-the-docker-image","title":"Invoke - Building the Docker Image","text":"<p>The first thing you need to do is build the necessary Docker image for Nautobot that installs the specific <code>nautobot_ver</code>. The image is used for Nautobot and the Celery worker service used by Docker Compose.</p> <pre><code>\u279c invoke build\n... &lt;omitted for brevity&gt;\n#14 exporting to image\n#14 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n#14 exporting layers\n#14 exporting layers 1.2s done\n#14 writing image sha256:2d524bc1665327faa0d34001b0a9d2ccf450612bf8feeb969312e96a2d3e3503 done\n#14 naming to docker.io/nautobot-graphql-observability/nautobot:3.0.0-py3.12 done\n</code></pre>"},{"location":"dev/dev_environment.html#invoke-starting-the-development-environment","title":"Invoke - Starting the Development Environment","text":"<p>Next, you need to start up your Docker containers.</p> <pre><code>\u279c invoke start\nStarting Nautobot in detached mode...\nRunning docker-compose command \"up --detach\"\nCreating network \"nautobot_graphql_observability_default\" with the default driver\nCreating volume \"nautobot_graphql_observability_postgres_data\" with default driver\nCreating nautobot_graphql_observability_redis_1 ...\nCreating nautobot_graphql_observability_docs_1  ...\nCreating nautobot_graphql_observability_postgres_1 ...\nCreating nautobot_graphql_observability_postgres_1 ... done\nCreating nautobot_graphql_observability_redis_1    ... done\nCreating nautobot_graphql_observability_nautobot_1 ...\nCreating nautobot_graphql_observability_docs_1     ... done\nCreating nautobot_graphql_observability_nautobot_1 ... done\nCreating nautobot_graphql_observability_worker_1   ...\nCreating nautobot_graphql_observability_worker_1   ... done\nDocker Compose is now in the Docker CLI, try `docker compose up`\n</code></pre> <p>This will start all of the Docker containers used for hosting Nautobot. You should see the following containers running after <code>invoke start</code> is finished.</p> <pre><code>\u279c docker ps\n****CONTAINER ID   IMAGE                            COMMAND                  CREATED          STATUS          PORTS                                       NAMES\nee90fbfabd77   nautobot-graphql-observability/nautobot:3.0.0-py3.12  \"nautobot-server rqw\u2026\"   16 seconds ago   Up 13 seconds                                               nautobot_graphql_observability_worker_1\nb8adb781d013   nautobot-graphql-observability/nautobot:3.0.0-py3.12  \"/docker-entrypoint.\u2026\"   20 seconds ago   Up 15 seconds   0.0.0.0:8080-&gt;8080/tcp, :::8080-&gt;8080/tcp   nautobot_graphql_observability_nautobot_1\nd64ebd60675d   nautobot-graphql-observability/nautobot:3.0.0-py3.12  \"mkdocs serve -v -a \u2026\"   25 seconds ago   Up 18 seconds   0.0.0.0:8001-&gt;8080/tcp, :::8001-&gt;8080/tcp   nautobot_graphql_observability_docs_1\ne72d63129b36   postgres:13-alpine               \"docker-entrypoint.s\u2026\"   25 seconds ago   Up 19 seconds   0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp   nautobot_graphql_observability_postgres_1\n96c6ff66997c   redis:6-alpine                   \"docker-entrypoint.s\u2026\"   25 seconds ago   Up 21 seconds   0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp   nautobot_graphql_observability_redis_1\n</code></pre> <p>Once the containers are fully up, you should be able to open up a web browser, and view:</p> <ul> <li>The Nautobot homepage at http://localhost:8080</li> <li>A live version of the documentation at http://localhost:8001</li> </ul> <p>Note</p> <p>Sometimes the containers take a minute to fully spin up. If the page doesn't load right away, wait a minute and try again.</p>"},{"location":"dev/dev_environment.html#invoke-creating-a-superuser","title":"Invoke - Creating a Superuser","text":"<p>The Nautobot development image will automatically provision a super user when specifying the following variables within <code>creds.env</code> which is the default when copying <code>creds.example.env</code> to <code>creds.env</code>.</p> <ul> <li><code>NAUTOBOT_CREATE_SUPERUSER=true</code></li> <li><code>NAUTOBOT_SUPERUSER_API_TOKEN=0123456789abcdef0123456789abcdef01234567</code></li> <li><code>NAUTOBOT_SUPERUSER_PASSWORD=admin</code></li> </ul> <p>Note</p> <p>The default username is admin, but can be overridden by specifying NAUTOBOT_SUPERUSER_USERNAME.</p> <p>If you need to create additional superusers, run the follow commands.</p> <pre><code>\u279c invoke createsuperuser\nRunning docker-compose command \"ps --services --filter status=running\"\nRunning docker-compose command \"exec nautobot nautobot-server createsuperuser --username admin\"\nError: That username is already taken.\nUsername: ntc\nEmail address: ntc@networktocode.com\nPassword:\nPassword (again):\nSuperuser created successfully.\n</code></pre>"},{"location":"dev/dev_environment.html#invoke-stopping-the-development-environment","title":"Invoke - Stopping the Development Environment","text":"<p>The last command to know for now is <code>invoke stop</code>.</p> <pre><code>\u279c invoke stop\nStopping Nautobot...\nRunning docker-compose command \"down\"\nStopping nautobot_graphql_observability_worker_1   ...\nStopping nautobot_graphql_observability_nautobot_1 ...\nStopping nautobot_graphql_observability_docs_1     ...\nStopping nautobot_graphql_observability_redis_1    ...\nStopping nautobot_graphql_observability_postgres_1 ...\nStopping nautobot_graphql_observability_worker_1   ... done\nStopping nautobot_graphql_observability_nautobot_1 ... done\nStopping nautobot_graphql_observability_postgres_1 ... done\nStopping nautobot_graphql_observability_redis_1    ... done\nStopping nautobot_graphql_observability_docs_1     ... done\nRemoving nautobot_graphql_observability_worker_1   ...\nRemoving nautobot_graphql_observability_nautobot_1 ...\nRemoving nautobot_graphql_observability_docs_1     ...\nRemoving nautobot_graphql_observability_redis_1    ...\nRemoving nautobot_graphql_observability_postgres_1 ...\nRemoving nautobot_graphql_observability_postgres_1 ... done\nRemoving nautobot_graphql_observability_docs_1     ... done\nRemoving nautobot_graphql_observability_worker_1   ... done\nRemoving nautobot_graphql_observability_redis_1    ... done\nRemoving nautobot_graphql_observability_nautobot_1 ... done\nRemoving network nautobot_graphql_observability_default\n</code></pre> <p>This will safely shut down all of your running Docker containers for this project. When you are ready to spin containers back up, it is as simple as running <code>invoke start</code> again as seen previously.</p> <p>Warning</p> <p>If you're wanting to reset the database and configuration settings, you can use the <code>invoke destroy</code> command, but you will lose any data stored in those containers, so make sure that is what you want to do.</p>"},{"location":"dev/dev_environment.html#real-time-updates-how-cool","title":"Real-Time Updates? How Cool!","text":"<p>Your environment should now be fully setup, all necessary Docker containers are created and running, and you're logged into Nautobot in your web browser. Now what?</p> <p>Now you can start developing your app in the project folder!</p> <p>The magic here is the root directory is mounted inside your Docker containers when built and ran, so any changes made to the files in here are directly updated to the Nautobot app code running in Docker. This means that as you modify the code in your app folder, the changes will be instantly updated in Nautobot.</p> <p>Warning</p> <p>There are a few exceptions to this, as outlined in the section To Rebuild or Not To Rebuild.</p> <p>The back-end Django process is setup to automatically reload itself (it only takes a couple of seconds) every time a file is updated (saved). So for example, if you were to update one of the files like <code>tables.py</code>, then save it, the changes will be visible right away in the web browser!</p> <p>Note</p> <p>You may get connection refused while Django reloads, but it should be refreshed fairly quickly.</p>"},{"location":"dev/dev_environment.html#docker-logs","title":"Docker Logs","text":"<p>When trying to debug an issue, one helpful thing you can look at are the logs within the Docker containers.</p> <pre><code>\u279c docker logs &lt;name of container&gt; -f\n</code></pre> <p>Note</p> <p>The <code>-f</code> tag will keep the logs open, and output them in realtime as they are generated.</p> <p>Info</p> <p>Want to limit the log output even further? Use the <code>--tail &lt;#&gt;</code> command line argument in conjunction with <code>-f</code>.</p> <p>So for example, our app is named <code>nautobot-graphql-observability</code>, the command would most likely be <code>docker logs nautobot_graphql_observability_nautobot_1 -f</code>. You can find the name of all running containers via <code>docker ps</code>.</p> <p>If you want to view the logs specific to the worker container, simply use the name of that container instead.</p>"},{"location":"dev/dev_environment.html#to-rebuild-or-not-to-rebuild","title":"To Rebuild or Not to Rebuild","text":"<p>Most of the time, you will not need to rebuild your images. Simply running <code>invoke start</code> and <code>invoke stop</code> is enough to keep your environment going.</p> <p>However there are a couple of instances when you will want to.</p>"},{"location":"dev/dev_environment.html#updating-environment-variables","title":"Updating Environment Variables","text":"<p>To add environment variables to your containers, thus allowing Nautobot to use them, you will update/add them in the <code>development/development.env</code> file. However, doing so is considered updating the underlying container shell, instead of Django (which auto restarts itself on changes).</p> <p>To get new environment variables to take effect, you will need stop any running images, rebuild the images, then restart them. This can easily be done with 3 commands:</p> <pre><code>\u279c invoke stop\n\u279c invoke build\n\u279c invoke start\n</code></pre> <p>Once completed, the new/updated environment variables should now be live.</p>"},{"location":"dev/dev_environment.html#installing-additional-python-packages","title":"Installing Additional Python Packages","text":"<p>If you want your app to leverage another available Nautobot app or another Python package, you can easily add them into your Docker environment.</p> <pre><code>\u279c poetry add &lt;package_name&gt;\n</code></pre> <p>Once the dependencies are resolved, stop the existing containers, rebuild the Docker image, and then start all containers again.</p> <pre><code>\u279c invoke stop\n\u279c invoke build\n\u279c invoke start\n</code></pre>"},{"location":"dev/dev_environment.html#installing-additional-nautobot-apps","title":"Installing Additional Nautobot Apps","text":"<p>Let's say for example you want the new app you're creating to integrate into Slack. To do this, you will want to integrate into the existing Nautobot ChatOps App.</p> <pre><code>\u279c poetry add nautobot-chatops\n</code></pre> <p>Once you activate the virtual environment via Poetry, you then tell Poetry to install the new app.</p> <p>Before you continue, you'll need to update the file <code>development/nautobot_config.py</code> accordingly with the name of the new app under <code>PLUGINS</code> and any relevant settings as necessary for the app under <code>PLUGINS_CONFIG</code>. Since you're modifying the underlying OS (not just Django files), you need to rebuild the image. This is a similar process to updating environment variables, which was explained earlier.</p> <pre><code>\u279c invoke stop\n\u279c invoke build\n\u279c invoke start\n</code></pre> <p>Once the containers are up and running, you should now see the new app installed in your Nautobot instance.</p> <p>Note</p> <p>You can even launch an <code>ngrok</code> service locally on your laptop, pointing to port 8080 (such as for chatops development), and it will point traffic directly to your Docker images.</p>"},{"location":"dev/dev_environment.html#updating-python-version","title":"Updating Python Version","text":"<p>To update the Python version, you can update it within <code>tasks.py</code>.</p> <pre><code>namespace = Collection(\"nautobot_graphql_observability\")\nnamespace.configure(\n    {\n        \"nautobot_graphql_observability\": {\n            ...\n            \"python_ver\": \"3.12\",\n        ...\n        }\n    }\n)\n</code></pre> <p>Or set the <code>INVOKE_NAUTOBOT_GRAPHQL_OBSERVABILITY_PYTHON_VER</code> variable.</p>"},{"location":"dev/dev_environment.html#updating-nautobot-version","title":"Updating Nautobot Version","text":"<p>To update the Nautobot version, you can update it within <code>tasks.py</code>.</p> <pre><code>namespace = Collection(\"nautobot_graphql_observability\")\nnamespace.configure(\n    {\n        \"nautobot_graphql_observability\": {\n            ...\n            \"nautobot_ver\": \"3.0.0\",\n        ...\n        }\n    }\n)\n</code></pre> <p>Or set the <code>INVOKE_NAUTOBOT_GRAPHQL_OBSERVABILITY_NAUTOBOT_VER</code> variable.</p>"},{"location":"dev/dev_environment.html#other-miscellaneous-commands-to-know","title":"Other Miscellaneous Commands To Know","text":""},{"location":"dev/dev_environment.html#python-shell","title":"Python Shell","text":"<p>To drop into a Django shell for Nautobot (in the Docker container) run:</p> <pre><code>\u279c invoke nbshell\n</code></pre> <p>This is the same as running:</p> <pre><code>\u279c invoke cli\n\u279c nautobot-server nbshell\n</code></pre>"},{"location":"dev/dev_environment.html#ipython-shell-plus","title":"iPython Shell Plus","text":"<p>Django also has a more advanced shell that uses iPython and that will automatically import all the models:</p> <pre><code>\u279c invoke shell-plus\n</code></pre> <p>This is the same as running:</p> <pre><code>\u279c invoke cli\n\u279c nautobot-server shell_plus\n</code></pre>"},{"location":"dev/dev_environment.html#tests","title":"Tests","text":"<p>To run tests against your code, you can run all of the tests that the CI runs against any new PR with:</p> <pre><code>\u279c invoke tests\n</code></pre> <p>To run an individual test, you can run any or all of the following:</p> <pre><code>\u279c invoke unittest\n\u279c invoke ruff\n\u279c invoke pylint\n</code></pre>"},{"location":"dev/dev_environment.html#app-configuration-schema","title":"App Configuration Schema","text":"<p>In the package source, there is the <code>nautobot_graphql_observability/app-config-schema.json</code> file, conforming to the JSON Schema format. This file is used to validate the configuration of the app in CI pipelines.</p> <p>If you make changes to <code>PLUGINS_CONFIG</code> or the configuration schema, you can run the following command to validate the schema:</p> <pre><code>invoke validate-app-config\n</code></pre> <p>To generate the <code>app-config-schema.json</code> file based on the current <code>PLUGINS_CONFIG</code> configuration, run the following command:</p> <pre><code>invoke generate-app-config-schema\n</code></pre> <p>This command can only guess the schema, so it's up to the developer to manually update the schema as needed.</p>"},{"location":"dev/extending.html","title":"Extending the App","text":"<p>Contributions and extensions are welcome. Please open an issue first to discuss the proposed change before submitting a PR.</p>"},{"location":"dev/extending.html#adding-custom-metrics","title":"Adding Custom Metrics","text":"<p>To add a new Prometheus metric:</p> <ol> <li> <p>Define the metric in <code>nautobot_graphql_observability/metrics.py</code>:</p> <pre><code>from prometheus_client import Counter\n\ngraphql_deprecated_fields_total = Counter(\n    \"graphql_deprecated_fields_total\",\n    \"Total usage of deprecated GraphQL fields\",\n    [\"type_name\", \"field_name\"],\n)\n</code></pre> </li> <li> <p>Import and record it in the appropriate method of <code>PrometheusMiddleware</code> in <code>nautobot_graphql_observability/middleware.py</code>.</p> </li> <li> <p>If the metric should be optional, add a new boolean setting to <code>NautobotAppGraphqlObservabilityConfig.default_settings</code> in <code>__init__.py</code> and gate the recording behind a config check in the middleware.</p> </li> </ol>"},{"location":"dev/extending.html#adding-new-labels-to-existing-metrics","title":"Adding New Labels to Existing Metrics","text":"<p>Adding labels to existing metrics is a breaking change for Prometheus (it creates a new time series). If you need additional labels:</p> <ol> <li>Consider creating a new metric instead.</li> <li>If modifying an existing metric, update the label list in <code>metrics.py</code> and all <code>.labels()</code> calls in <code>middleware.py</code>.</li> <li>Update tests to include the new label values.</li> </ol>"},{"location":"dev/extending.html#customizing-histogram-buckets","title":"Customizing Histogram Buckets","text":"<p>The default histogram buckets are defined in <code>metrics.py</code>. To customize them for your deployment, you can fork the metric definitions. A future enhancement may allow bucket configuration via <code>PLUGINS_CONFIG</code>.</p>"},{"location":"dev/extending.html#extending-the-logging-middleware","title":"Extending the Logging Middleware","text":"<p>The <code>GraphQLQueryLoggingMiddleware</code> in <code>nautobot_graphql_observability/logging_middleware.py</code> can be extended to add custom fields to log entries. The middleware uses Python's standard <code>logging</code> module with the logger name <code>nautobot_graphql_observability.graphql_query_log</code>.</p> <p>To add custom log fields, subclass <code>GraphQLQueryLoggingMiddleware</code> and override <code>_log_query()</code>:</p> <pre><code>from nautobot_graphql_observability.logging_middleware import GraphQLQueryLoggingMiddleware\n\nclass CustomLoggingMiddleware(GraphQLQueryLoggingMiddleware):\n    @staticmethod\n    def _log_query(config, operation_type, operation_name, user, start_time, info, error=None):\n        # Call parent to emit the standard log entry\n        GraphQLQueryLoggingMiddleware._log_query(\n            config, operation_type, operation_name, user, start_time, info, error\n        )\n        # Add custom logic here\n</code></pre>"},{"location":"dev/release_checklist.html","title":"Release Checklist","text":"<p>This document is intended for app maintainers and outlines the steps to perform when releasing a new version of the app.</p> <p>Important</p> <p>Before starting, make sure your local <code>develop</code>, <code>main</code>, and (if applicable) the current LTM branch are all up to date with upstream!</p> <pre><code>git fetch\ngit switch develop &amp;&amp; git pull # and repeat for main/ltm\n</code></pre> <p>Choose your own adventure:</p> <ul> <li>LTM release? Jump here.</li> <li>Patch release from <code>develop</code>? Jump here.</li> <li>Minor release? Continue with Minor Version Bumps and then All Releases from <code>develop</code>.</li> </ul>"},{"location":"dev/release_checklist.html#minor-version-bumps","title":"Minor Version Bumps","text":""},{"location":"dev/release_checklist.html#update-requirements","title":"Update Requirements","text":"<p>Every minor version release should refresh <code>poetry.lock</code>, so that it lists the most recent stable release of each package. To do this:</p> <ol> <li>Run <code>poetry update --dry-run</code> to have Poetry automatically tell you what package updates are available and the versions it would upgrade to. This requires an existing environment created from the lock file (i.e. via <code>poetry install</code>).</li> <li>Review each requirement's release notes for any breaking or otherwise noteworthy changes.</li> <li>Run <code>poetry update &lt;package&gt;</code> to update the package versions in <code>poetry.lock</code> as appropriate.</li> <li>If a required package requires updating to a new release not covered in the version constraints for a package as defined in <code>pyproject.toml</code>, (e.g. <code>Django ~3.1.7</code> would never install <code>Django &gt;=4.0.0</code>), update it manually in <code>pyproject.toml</code>.</li> <li>Run <code>poetry install</code> to install the refreshed versions of all required packages.</li> <li>Run all tests (<code>poetry run invoke tests</code>) and check that the UI and API function as expected.</li> </ol>"},{"location":"dev/release_checklist.html#update-documentation","title":"Update Documentation","text":"<p>If there are any changes to the compatibility matrix (such as a bump in the minimum supported Nautobot version), update it accordingly.</p> <p>Commit any resulting changes from the following sections to the documentation before proceeding with the release.</p> <p>Tip</p> <p>Fire up the documentation server in your development environment with <code>poetry run mkdocs serve</code>! This allows you to view the documentation site locally (the link is in the output of the command) and automatically rebuilds it as you make changes.</p>"},{"location":"dev/release_checklist.html#verify-the-installation-and-upgrade-steps","title":"Verify the Installation and Upgrade Steps","text":"<p>Follow the installation instructions to perform a new production installation of the app. If possible, also test the upgrade process from the previous released version.</p> <p>The goal of this step is to walk through the entire install process as documented to make sure nothing there needs to be changed or updated, to catch any errors or omissions in the documentation, and to ensure that it is current with each release.</p>"},{"location":"dev/release_checklist.html#all-releases-from-develop","title":"All Releases from <code>develop</code>","text":""},{"location":"dev/release_checklist.html#verify-ci-build-status","title":"Verify CI Build Status","text":"<p>Ensure that continuous integration testing on the <code>develop</code> branch is completing successfully.</p>"},{"location":"dev/release_checklist.html#bump-the-version","title":"Bump the Version","text":"<p>Update the package version using <code>poetry version</code> if necessary (poetry docs). This command shows the current version of the project or bumps the version of the project and writes the new version back to <code>pyproject.toml</code> if a valid bump rule is provided.</p> <p>The new version must be a valid semver string or a valid bump rule: <code>patch</code>, <code>minor</code>, <code>major</code>, <code>prepatch</code>, <code>preminor</code>, <code>premajor</code>, <code>prerelease</code>. Always try to use a bump rule when you can.</p> <p>Warning</p> <p>This guide uses <code>1.4.2</code> as the new version in its examples, so change it to match the version you bumped to in the previous step! Every. single. time. you. copy/paste commands!</p> <p>Display the current version with no arguments:</p> <pre><code>&gt; poetry version\nnautobot-graphql-observability 1.0.0-beta.2\n</code></pre> <p>Bump pre-release versions using <code>prerelease</code>:</p> <pre><code>&gt; poetry version prerelease\nBumping version from 1.0.0-beta.2 to 1.0.0-beta.3\n</code></pre> <p>For major versions, use <code>major</code>:</p> <pre><code>&gt; poetry version major\nBumping version from 1.0.0-beta.2 to 1.0.0\n</code></pre> <p>For patch versions, use <code>minor</code>:</p> <pre><code>&gt; poetry version minor\nBumping version from 1.0.0 to 1.1.0\n</code></pre> <p>And lastly, for patch versions, you guessed it, use <code>patch</code>:</p> <pre><code>&gt; poetry version patch\nBumping version from 1.1.0 to 1.1.1\n</code></pre>"},{"location":"dev/release_checklist.html#update-the-changelog","title":"Update the Changelog","text":"<p>Note</p> <ul> <li>This project uses <code>towncrier</code> to track human readable changes, so all merged PRs will have one or more entries in the release notes.</li> <li>The changelog must adhere to the Keep a Changelog style guide for any manual changes you may need to make.</li> <li>You will need to have the project's poetry environment built at this stage, as the towncrier command runs locally only. If you don't have it, run <code>poetry install</code> first.</li> <li>You can also set the version explicitly with <code>invoke generate-release-notes --version 1.4.2</code> if it needs to be different from what's in <code>pyproject.toml</code>.</li> </ul> <p>First, create a release branch off of <code>develop</code> (<code>git switch -c release-1.4.2 develop</code>) and automatically generate release notes with <code>invoke generate-release-notes</code>.</p> <p>If you're releasing a new major or minor version, this will create a new <code>docs/admin/release_notes/version_{major}.{minor}.md</code> file. Please fill in the <code>Release Overview</code> section in that file manually with a user-friendly summary of the most notable changes!</p> <p>Stage any remaining files (e.g. <code>git add mkdocs.yml pyproject.toml</code>) and check the diffs to verify all of the changes are correct (<code>git diff --cached</code>). For a new release of <code>1.4.2</code>, this will update the release notes in <code>docs/admin/release_notes/version_1.4.md</code>, stage that file in git, and <code>git rm</code> all the fragments that have now been incorporated into the release notes.</p> <p>Commit <code>git commit -m \"Release v1.4.2\"</code> and <code>git push</code> the staged changes.</p>"},{"location":"dev/release_checklist.html#submit-release-pull-request","title":"Submit Release Pull Request","text":"<p>Submit a pull request titled <code>Release v1.4.2</code> to merge your release branch into <code>main</code>. Copy the documented release notes into the pull request's body.</p> <p>Important</p> <p>Do not squash merge this branch into <code>main</code>. Make sure to select <code>Create a merge commit</code> when merging in GitHub.</p> <p>Once CI has completed on the PR, merge it.</p>"},{"location":"dev/release_checklist.html#create-a-new-release-in-github","title":"Create a New Release in GitHub","text":"<p>Draft a new release with the following parameters.</p> <ul> <li>Tag: Input current version (e.g. <code>v1.4.2</code>) and select <code>Create new tag: v1.4.2 on publish</code></li> <li>Target: <code>main</code></li> <li>Title: Version and date (e.g. <code>v1.4.2 - 2024-04-02</code>)</li> </ul> <p>Click \"Generate Release Notes\" and edit the auto-generated content as follows:</p> <ul> <li>Change the entries generated by GitHub to only the usernames of the contributors. e.g. <code>* Updated dockerfile by @nautobot_user in https://github.com/slydien/nautobot-app-graphql-observability/pull/123</code> -&gt; <code>* @nautobot_user</code>.<ul> <li>This should give you the list for the new <code>Contributors</code> section.</li> <li>Make sure there are no duplicated entries.</li> </ul> </li> <li>Replace the content of the <code>What's Changed</code> section with the description of changes from the release PR (what towncrier generated).</li> <li>If it exists, leave the <code>New Contributors</code> list as it is.</li> </ul> <p>The release notes should look as follows:</p> <pre><code>## What's Changed\n\n**Towncrier generated Changed/Fixed/Housekeeping etc. sections here**\n\n## Contributors\n\n* @alice\n* @bob\n\n## New Contributors\n\n* @bob\n\n**Full Changelog**: https://github.com/slydien/nautobot-app-graphql-observability/compare/v1.4.1...v1.4.2\n</code></pre> <p>Publish the release!</p>"},{"location":"dev/release_checklist.html#create-a-pr-from-main-back-to-develop","title":"Create a PR from <code>main</code> back to <code>develop</code>","text":"<p>First, sync your <code>main</code> branch with upstream changes: <code>git switch main &amp;&amp; git pull</code>.</p> <p>Create a new branch from <code>main</code> called <code>release-1.4.2-to-develop</code> and use <code>poetry version prepatch</code> to bump the development version to the next release.</p> <p>For example, if you just released <code>v1.4.2</code>:</p> <pre><code>&gt; git switch -c release-1.4.2-to-develop main\nSwitched to a new branch 'release-1.4.2-to-develop'\n\n&gt; poetry version prepatch\nBumping version from 1.4.2 to 1.4.3a1\n\n&gt; git add pyproject.toml &amp;&amp; git commit -m \"Bump version\"\n\n&gt; git push\n</code></pre> <p>Important</p> <p>Do not squash merge this branch into <code>develop</code>. Make sure to select <code>Create a merge commit</code> when merging in GitHub.</p> <p>Open a new PR from <code>release-1.4.2-to-develop</code> against <code>develop</code>, wait for CI to pass, and merge it.</p>"},{"location":"dev/release_checklist.html#final-checks","title":"Final checks","text":"<p>At this stage, the CI should be running or finished for the <code>v1.4.2</code> tag and a package successfully published to PyPI and added into the GitHub Release. Double check that's the case.</p> <p>Documentation should also have been built for the tag on ReadTheDocs and if you're reading this page online, refresh it and look for the new version in the little version fly-out menu down at the bottom right of the page.</p> <p>All done!</p>"},{"location":"dev/release_checklist.html#ltm-releases","title":"LTM Releases","text":"<p>For projects maintaining a Nautobot LTM compatible release, all development and release management is done through the <code>ltm-x.y</code> branch. The <code>x.y</code> relates to the LTM version of Nautobot it's compatible with, for example <code>1.6</code>.</p> <p>The process is similar to releasing from <code>develop</code>, but there is no need for post-release branch syncing because you'll release directly from the LTM branch:</p> <ol> <li>Make sure your <code>ltm-1.6</code> branch is passing CI.</li> <li>Create a release branch from the <code>ltm-1.6</code> branch: <code>git switch -c release-1.2.3 ltm-1.6</code>.</li> <li>Bump up the patch version <code>poetry version patch</code>. If you're backporting a feature instead of bugfixes, bump the minor version instead with <code>poetry version minor</code>.</li> <li>Generate the release notes: <code>invoke generate-release-notes --version 1.2.3</code>.</li> <li>Move the release notes from the generated <code>docs/admin/release_notes/version_X.Y.md</code> to <code>docs/admin/release_notes/version_1.2.md</code>.</li> <li>Add all the changes and <code>git commit -m \"Release v1.2.3\"</code>, then <code>git push</code>.</li> <li>Open a new PR against <code>ltm-1.6</code>. Once CI is passing in the PR, <code>Create a merge commit</code> (don't squash!).</li> <li>Create a New Release in GitHub - use the same steps documented here.</li> <li>Open a separate PR against <code>develop</code> to synchronize all LTM release changelogs into the latest version of the docs for visibility.</li> </ol>"},{"location":"dev/code_reference/index.html","title":"Code Reference","text":"<p>Auto-generated code reference documentation from docstrings, using mkdocstrings.</p> <ul> <li>API - REST API views and serializers.</li> <li>Package - Core package modules (middleware, metrics, utilities).</li> </ul>"},{"location":"dev/code_reference/api.html","title":"Views and URL Configuration","text":""},{"location":"dev/code_reference/api.html#nautobot_graphql_observability.views","title":"<code>nautobot_graphql_observability.views</code>","text":"<p>Views for the nautobot_graphql_observability app.</p>"},{"location":"dev/code_reference/api.html#nautobot_graphql_observability.urls","title":"<code>nautobot_graphql_observability.urls</code>","text":"<p>Django urlpatterns declaration for nautobot_graphql_observability app.</p>"},{"location":"dev/code_reference/package.html","title":"Package","text":""},{"location":"dev/code_reference/package.html#nautobot_graphql_observability","title":"<code>nautobot_graphql_observability</code>","text":"<p>App declaration for nautobot_graphql_observability.</p>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.NautobotAppGraphqlObservabilityConfig","title":"<code>NautobotAppGraphqlObservabilityConfig</code>","text":"<p>               Bases: <code>NautobotAppConfig</code></p> <p>App configuration for the nautobot_graphql_observability app.</p> Source code in <code>nautobot_graphql_observability/__init__.py</code> <pre><code>class NautobotAppGraphqlObservabilityConfig(NautobotAppConfig):\n    \"\"\"App configuration for the nautobot_graphql_observability app.\"\"\"\n\n    name = \"nautobot_graphql_observability\"\n    verbose_name = \"Nautobot App GraphQL Observability\"\n    version = __version__\n    author = \"Lydien SANDANASAMY\"\n    description = \"Nautobot App GraphQL Observability.\"\n    base_url = \"nautobot-graphql-observability\"\n    required_settings = []\n    default_settings = {\n        \"graphql_metrics_enabled\": True,\n        \"track_query_depth\": True,\n        \"track_query_complexity\": True,\n        \"track_field_resolution\": False,\n        \"track_per_user\": True,\n        \"query_logging_enabled\": False,\n        \"log_query_body\": False,\n        \"log_query_variables\": False,\n    }\n    middleware = [\n        \"nautobot_graphql_observability.django_middleware.GraphQLObservabilityDjangoMiddleware\",\n    ]\n    docs_view_name = \"plugins:nautobot_graphql_observability:docs\"\n    searchable_models = []\n\n    def ready(self):\n        \"\"\"Patch Nautobot's GraphQLDRFAPIView to load Graphene middleware from settings.\n\n        Nautobot's ``GraphQLDRFAPIView.init_graphql()`` does not load middleware\n        from ``GRAPHENE[\"MIDDLEWARE\"]`` when ``self.middleware`` is ``None`` (the\n        default).  This is a known limitation of the DRF-based GraphQL view \u2014\n        the standard ``graphene_django.views.GraphQLView`` (used by the GraphiQL\n        UI at ``/graphql/``) loads middleware correctly.\n\n        No official extension point (``override_views``, etc.) can replace this\n        patch because the ``graphql-api`` URL is registered without a namespace.\n        Request duration and query logging are handled by\n        :class:`~nautobot_graphql_observability.django_middleware.GraphQLObservabilityDjangoMiddleware`,\n        which is registered via :attr:`middleware` (the official Nautobot mechanism).\n        \"\"\"\n        super().ready()\n        self._patch_init_graphql()\n\n    @staticmethod\n    def _patch_init_graphql():\n        \"\"\"Patch ``GraphQLDRFAPIView.init_graphql`` to load ``GRAPHENE[\"MIDDLEWARE\"]``.\"\"\"\n        from nautobot.core.api.views import GraphQLDRFAPIView  # pylint: disable=import-outside-toplevel\n\n        original_init_graphql = GraphQLDRFAPIView.init_graphql\n\n        def patched_init_graphql(view_self):\n            original_init_graphql(view_self)\n            if view_self.middleware is None:\n                from graphene_django.settings import graphene_settings  # pylint: disable=import-outside-toplevel\n                from graphene_django.views import instantiate_middleware  # pylint: disable=import-outside-toplevel\n\n                if graphene_settings.MIDDLEWARE:\n                    view_self.middleware = list(instantiate_middleware(graphene_settings.MIDDLEWARE))\n\n        GraphQLDRFAPIView.init_graphql = patched_init_graphql\n</code></pre>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.NautobotAppGraphqlObservabilityConfig.ready","title":"<code>ready()</code>","text":"<p>Patch Nautobot's GraphQLDRFAPIView to load Graphene middleware from settings.</p> <p>Nautobot's <code>GraphQLDRFAPIView.init_graphql()</code> does not load middleware from <code>GRAPHENE[\"MIDDLEWARE\"]</code> when <code>self.middleware</code> is <code>None</code> (the default).  This is a known limitation of the DRF-based GraphQL view \u2014 the standard <code>graphene_django.views.GraphQLView</code> (used by the GraphiQL UI at <code>/graphql/</code>) loads middleware correctly.</p> <p>No official extension point (<code>override_views</code>, etc.) can replace this patch because the <code>graphql-api</code> URL is registered without a namespace. Request duration and query logging are handled by :class:<code>~nautobot_graphql_observability.django_middleware.GraphQLObservabilityDjangoMiddleware</code>, which is registered via :attr:<code>middleware</code> (the official Nautobot mechanism).</p> Source code in <code>nautobot_graphql_observability/__init__.py</code> <pre><code>def ready(self):\n    \"\"\"Patch Nautobot's GraphQLDRFAPIView to load Graphene middleware from settings.\n\n    Nautobot's ``GraphQLDRFAPIView.init_graphql()`` does not load middleware\n    from ``GRAPHENE[\"MIDDLEWARE\"]`` when ``self.middleware`` is ``None`` (the\n    default).  This is a known limitation of the DRF-based GraphQL view \u2014\n    the standard ``graphene_django.views.GraphQLView`` (used by the GraphiQL\n    UI at ``/graphql/``) loads middleware correctly.\n\n    No official extension point (``override_views``, etc.) can replace this\n    patch because the ``graphql-api`` URL is registered without a namespace.\n    Request duration and query logging are handled by\n    :class:`~nautobot_graphql_observability.django_middleware.GraphQLObservabilityDjangoMiddleware`,\n    which is registered via :attr:`middleware` (the official Nautobot mechanism).\n    \"\"\"\n    super().ready()\n    self._patch_init_graphql()\n</code></pre>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.middleware","title":"<code>nautobot_graphql_observability.middleware</code>","text":"<p>Graphene middleware for exporting Prometheus metrics from GraphQL queries.</p>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.middleware.PrometheusMiddleware","title":"<code>PrometheusMiddleware</code>","text":"<p>Graphene middleware that instruments GraphQL resolvers with Prometheus metrics.</p> <p>On root-level resolutions, records counters and advanced metrics immediately (these are not timing-sensitive) and stashes operation labels onto the request so that :class:<code>PrometheusDjangoMiddleware</code> can record the duration histogram after the full HTTP response is built.</p> <p>Optionally records advanced metrics based on app configuration:</p> <ul> <li><code>track_query_depth</code>: Record query nesting depth histogram.</li> <li><code>track_query_complexity</code>: Record query field count histogram.</li> <li><code>track_field_resolution</code>: Record per-field resolver duration histogram.</li> <li><code>track_per_user</code>: Record per-user request counter.</li> </ul> <p>Usage in Django settings::</p> <pre><code>GRAPHENE = {\n    \"MIDDLEWARE\": [\n        \"nautobot_graphql_observability.middleware.PrometheusMiddleware\",\n    ]\n}\n</code></pre> Source code in <code>nautobot_graphql_observability/middleware.py</code> <pre><code>class PrometheusMiddleware:  # pylint: disable=too-few-public-methods\n    \"\"\"Graphene middleware that instruments GraphQL resolvers with Prometheus metrics.\n\n    On root-level resolutions, records counters and advanced metrics immediately\n    (these are not timing-sensitive) and stashes operation labels onto the\n    request so that :class:`PrometheusDjangoMiddleware` can record the duration\n    histogram after the full HTTP response is built.\n\n    Optionally records advanced metrics based on app configuration:\n\n    - ``track_query_depth``: Record query nesting depth histogram.\n    - ``track_query_complexity``: Record query field count histogram.\n    - ``track_field_resolution``: Record per-field resolver duration histogram.\n    - ``track_per_user``: Record per-user request counter.\n\n    Usage in Django settings::\n\n        GRAPHENE = {\n            \"MIDDLEWARE\": [\n                \"nautobot_graphql_observability.middleware.PrometheusMiddleware\",\n            ]\n        }\n    \"\"\"\n\n    def resolve(self, next: callable, root: object, info: GraphQLResolveInfo, **kwargs: object) -&gt; object:  # pylint: disable=redefined-builtin\n        \"\"\"Intercept each field resolution and record metrics.\n\n        Root-level resolutions (root is None) record counters and advanced\n        metrics and stash labels for the Django middleware to record duration.\n        Nested resolutions optionally record per-field duration when enabled.\n\n        Args:\n            next (callable): Callable to continue the resolution chain.\n            root (object): Parent resolved value. None for top-level fields.\n            info (GraphQLResolveInfo): GraphQL resolve info containing operation metadata.\n            **kwargs (object): Field arguments.\n\n        Returns:\n            object: The result of the resolver.\n        \"\"\"\n        config = _get_app_settings()\n\n        if root is not None:\n            if config.get(\"track_field_resolution\", False):\n                return self._resolve_field_with_metrics(next, root, info, **kwargs)\n            return next(root, info, **kwargs)\n\n        operation_type = info.operation.operation.value\n        operation_name = self._get_operation_name(info)\n\n        # Stash labels on the request (only for the first root field) so\n        # the Django middleware can record the full-request duration.\n        # For DRF views, info.context is a DRF Request wrapping a WSGIRequest.\n        # The Django middleware sees the WSGIRequest, so stash on both.\n        request = info.context\n        if not hasattr(request, _REQUEST_ATTR):\n            meta = {\n                \"operation_type\": operation_type,\n                \"operation_name\": operation_name,\n            }\n            stash_meta_on_request(request, _REQUEST_ATTR, meta)\n\n        try:\n            result = next(root, info, **kwargs)\n            return result\n        except Exception as error:\n            graphql_errors_total.labels(\n                operation_type=operation_type,\n                operation_name=operation_name,\n                error_type=type(error).__name__,\n            ).inc()\n            # Mark the error on the stashed metadata so the Django middleware\n            # records the correct status.\n            meta = getattr(request, _REQUEST_ATTR, None)\n            if meta is not None:\n                meta[\"error\"] = True\n            raise\n        finally:\n            # Counters and advanced metrics are not timing-sensitive, record now.\n            status = \"error\" if getattr(request, _REQUEST_ATTR, {}).get(\"error\") else \"success\"\n\n            graphql_requests_total.labels(\n                operation_type=operation_type,\n                operation_name=operation_name,\n                status=status,\n            ).inc()\n\n            self._record_advanced_metrics(info, operation_name, config)\n\n    @staticmethod\n    def _resolve_field_with_metrics(next, root, info, **kwargs):  # pylint: disable=redefined-builtin\n        \"\"\"Resolve a nested field while recording per-field duration.\"\"\"\n        type_name = info.parent_type.name if info.parent_type else \"Unknown\"\n        field_name = info.field_name\n\n        start_time = time.monotonic()\n        try:\n            return next(root, info, **kwargs)\n        finally:\n            duration = time.monotonic() - start_time\n            graphql_field_resolution_duration_seconds.labels(\n                type_name=type_name,\n                field_name=field_name,\n            ).observe(duration)\n\n    @staticmethod\n    def _record_advanced_metrics(info, operation_name, config):\n        \"\"\"Record query depth, complexity, and per-user metrics if enabled.\"\"\"\n        if config.get(\"track_query_depth\", True):\n            depth = calculate_query_depth(info.operation.selection_set, info.fragments)\n            graphql_query_depth.labels(operation_name=operation_name).observe(depth)\n\n        if config.get(\"track_query_complexity\", True):\n            complexity = calculate_query_complexity(info.operation.selection_set, info.fragments)\n            graphql_query_complexity.labels(operation_name=operation_name).observe(complexity)\n\n        if config.get(\"track_per_user\", True):\n            user = \"anonymous\"\n            request = info.context\n            if hasattr(request, \"user\") and hasattr(request.user, \"is_authenticated\"):\n                if request.user.is_authenticated:\n                    user = request.user.username\n            graphql_requests_by_user_total.labels(\n                user=user,\n                operation_type=info.operation.operation.value,\n                operation_name=operation_name,\n            ).inc()\n\n    @staticmethod\n    def _get_operation_name(info: GraphQLResolveInfo) -&gt; str:\n        \"\"\"Extract the operation name from the GraphQL query.\n\n        Uses the explicit operation name if provided, otherwise falls back\n        to the sorted, comma-joined root field names (e.g. \"devices,locations\").\n        \"\"\"\n        if info.operation.name:\n            return info.operation.name.value\n        root_fields = []\n        if info.operation.selection_set:\n            for selection in info.operation.selection_set.selections:\n                if isinstance(selection, FieldNode):\n                    root_fields.append(selection.name.value)\n        return \",\".join(sorted(root_fields)) if root_fields else \"anonymous\"\n</code></pre>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.middleware.PrometheusMiddleware.resolve","title":"<code>resolve(next, root, info, **kwargs)</code>","text":"<p>Intercept each field resolution and record metrics.</p> <p>Root-level resolutions (root is None) record counters and advanced metrics and stash labels for the Django middleware to record duration. Nested resolutions optionally record per-field duration when enabled.</p> <p>Parameters:</p> Name Type Description Default <code>next</code> <code>callable</code> <p>Callable to continue the resolution chain.</p> required <code>root</code> <code>object</code> <p>Parent resolved value. None for top-level fields.</p> required <code>info</code> <code>GraphQLResolveInfo</code> <p>GraphQL resolve info containing operation metadata.</p> required <code>**kwargs</code> <code>object</code> <p>Field arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The result of the resolver.</p> Source code in <code>nautobot_graphql_observability/middleware.py</code> <pre><code>def resolve(self, next: callable, root: object, info: GraphQLResolveInfo, **kwargs: object) -&gt; object:  # pylint: disable=redefined-builtin\n    \"\"\"Intercept each field resolution and record metrics.\n\n    Root-level resolutions (root is None) record counters and advanced\n    metrics and stash labels for the Django middleware to record duration.\n    Nested resolutions optionally record per-field duration when enabled.\n\n    Args:\n        next (callable): Callable to continue the resolution chain.\n        root (object): Parent resolved value. None for top-level fields.\n        info (GraphQLResolveInfo): GraphQL resolve info containing operation metadata.\n        **kwargs (object): Field arguments.\n\n    Returns:\n        object: The result of the resolver.\n    \"\"\"\n    config = _get_app_settings()\n\n    if root is not None:\n        if config.get(\"track_field_resolution\", False):\n            return self._resolve_field_with_metrics(next, root, info, **kwargs)\n        return next(root, info, **kwargs)\n\n    operation_type = info.operation.operation.value\n    operation_name = self._get_operation_name(info)\n\n    # Stash labels on the request (only for the first root field) so\n    # the Django middleware can record the full-request duration.\n    # For DRF views, info.context is a DRF Request wrapping a WSGIRequest.\n    # The Django middleware sees the WSGIRequest, so stash on both.\n    request = info.context\n    if not hasattr(request, _REQUEST_ATTR):\n        meta = {\n            \"operation_type\": operation_type,\n            \"operation_name\": operation_name,\n        }\n        stash_meta_on_request(request, _REQUEST_ATTR, meta)\n\n    try:\n        result = next(root, info, **kwargs)\n        return result\n    except Exception as error:\n        graphql_errors_total.labels(\n            operation_type=operation_type,\n            operation_name=operation_name,\n            error_type=type(error).__name__,\n        ).inc()\n        # Mark the error on the stashed metadata so the Django middleware\n        # records the correct status.\n        meta = getattr(request, _REQUEST_ATTR, None)\n        if meta is not None:\n            meta[\"error\"] = True\n        raise\n    finally:\n        # Counters and advanced metrics are not timing-sensitive, record now.\n        status = \"error\" if getattr(request, _REQUEST_ATTR, {}).get(\"error\") else \"success\"\n\n        graphql_requests_total.labels(\n            operation_type=operation_type,\n            operation_name=operation_name,\n            status=status,\n        ).inc()\n\n        self._record_advanced_metrics(info, operation_name, config)\n</code></pre>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.logging_middleware","title":"<code>nautobot_graphql_observability.logging_middleware</code>","text":"<p>Graphene middleware for logging GraphQL queries via Python's logging module.</p>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.logging_middleware.GraphQLQueryLoggingMiddleware","title":"<code>GraphQLQueryLoggingMiddleware</code>","text":"<p>Graphene middleware that captures GraphQL query metadata for logging.</p> <p>On root-level resolutions, stashes operation metadata onto the request so that :class:<code>GraphQLQueryLoggingDjangoMiddleware</code> can emit a log entry with the real total request duration after the full response is built.</p> <p>Controlled by app settings:</p> <ul> <li><code>query_logging_enabled</code>: Master switch (default: False).</li> <li><code>log_query_body</code>: Include the full query text (default: False).</li> <li><code>log_query_variables</code>: Include query variables (default: False).</li> </ul> <p>Usage in Django settings::</p> <pre><code>GRAPHENE = {\n    \"MIDDLEWARE\": [\n        \"nautobot_graphql_observability.logging_middleware.GraphQLQueryLoggingMiddleware\",\n        \"nautobot_graphql_observability.middleware.PrometheusMiddleware\",\n    ]\n}\n</code></pre> Source code in <code>nautobot_graphql_observability/logging_middleware.py</code> <pre><code>class GraphQLQueryLoggingMiddleware:  # pylint: disable=too-few-public-methods\n    \"\"\"Graphene middleware that captures GraphQL query metadata for logging.\n\n    On root-level resolutions, stashes operation metadata onto the request so\n    that :class:`GraphQLQueryLoggingDjangoMiddleware` can emit a log entry\n    with the **real** total request duration after the full response is built.\n\n    Controlled by app settings:\n\n    - ``query_logging_enabled``: Master switch (default: False).\n    - ``log_query_body``: Include the full query text (default: False).\n    - ``log_query_variables``: Include query variables (default: False).\n\n    Usage in Django settings::\n\n        GRAPHENE = {\n            \"MIDDLEWARE\": [\n                \"nautobot_graphql_observability.logging_middleware.GraphQLQueryLoggingMiddleware\",\n                \"nautobot_graphql_observability.middleware.PrometheusMiddleware\",\n            ]\n        }\n    \"\"\"\n\n    def resolve(self, next: callable, root: object, info: GraphQLResolveInfo, **kwargs: object) -&gt; object:  # pylint: disable=redefined-builtin\n        \"\"\"Intercept root-level resolutions and stash metadata on the request.\n\n        Args:\n            next (callable): Callable to continue the resolution chain.\n            root (object): Parent resolved value. None for top-level fields.\n            info (GraphQLResolveInfo): GraphQL resolve info containing operation metadata.\n            **kwargs (object): Field arguments.\n\n        Returns:\n            object: The result of the resolver.\n        \"\"\"\n        if root is not None:\n            return next(root, info, **kwargs)\n\n        config = _get_app_settings()\n        if not config.get(\"query_logging_enabled\", False):\n            return next(root, info, **kwargs)\n\n        # Stash metadata on the request (only for the first root field).\n        # For DRF views, info.context is a DRF Request wrapping a WSGIRequest.\n        # The Django middleware sees the WSGIRequest, so stash on both.\n        request = info.context\n        if not hasattr(request, _REQUEST_ATTR):\n            meta = {\n                \"operation_type\": info.operation.operation.value,\n                \"operation_name\": PrometheusMiddleware._get_operation_name(info),\n                \"user\": self._get_user(info),\n                \"config\": config,\n            }\n\n            if config.get(\"log_query_body\", False):\n                meta[\"query_body\"] = _extract_query_body(info)\n\n            if config.get(\"log_query_variables\", False):\n                meta[\"variables\"] = _extract_variables(info)\n\n            stash_meta_on_request(request, _REQUEST_ATTR, meta)\n\n        try:\n            return next(root, info, **kwargs)\n        except Exception as error:\n            # Record the error on the stashed metadata so the Django\n            # middleware can log it.\n            meta = getattr(request, _REQUEST_ATTR, None)\n            if meta is not None:\n                meta[\"error\"] = error\n            raise\n\n    @staticmethod\n    def _get_user(info):\n        \"\"\"Extract the username from the request context.\"\"\"\n        request = info.context\n        if hasattr(request, \"user\") and hasattr(request.user, \"is_authenticated\"):\n            if request.user.is_authenticated:\n                return request.user.username\n        return \"anonymous\"\n</code></pre>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.logging_middleware.GraphQLQueryLoggingMiddleware.resolve","title":"<code>resolve(next, root, info, **kwargs)</code>","text":"<p>Intercept root-level resolutions and stash metadata on the request.</p> <p>Parameters:</p> Name Type Description Default <code>next</code> <code>callable</code> <p>Callable to continue the resolution chain.</p> required <code>root</code> <code>object</code> <p>Parent resolved value. None for top-level fields.</p> required <code>info</code> <code>GraphQLResolveInfo</code> <p>GraphQL resolve info containing operation metadata.</p> required <code>**kwargs</code> <code>object</code> <p>Field arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The result of the resolver.</p> Source code in <code>nautobot_graphql_observability/logging_middleware.py</code> <pre><code>def resolve(self, next: callable, root: object, info: GraphQLResolveInfo, **kwargs: object) -&gt; object:  # pylint: disable=redefined-builtin\n    \"\"\"Intercept root-level resolutions and stash metadata on the request.\n\n    Args:\n        next (callable): Callable to continue the resolution chain.\n        root (object): Parent resolved value. None for top-level fields.\n        info (GraphQLResolveInfo): GraphQL resolve info containing operation metadata.\n        **kwargs (object): Field arguments.\n\n    Returns:\n        object: The result of the resolver.\n    \"\"\"\n    if root is not None:\n        return next(root, info, **kwargs)\n\n    config = _get_app_settings()\n    if not config.get(\"query_logging_enabled\", False):\n        return next(root, info, **kwargs)\n\n    # Stash metadata on the request (only for the first root field).\n    # For DRF views, info.context is a DRF Request wrapping a WSGIRequest.\n    # The Django middleware sees the WSGIRequest, so stash on both.\n    request = info.context\n    if not hasattr(request, _REQUEST_ATTR):\n        meta = {\n            \"operation_type\": info.operation.operation.value,\n            \"operation_name\": PrometheusMiddleware._get_operation_name(info),\n            \"user\": self._get_user(info),\n            \"config\": config,\n        }\n\n        if config.get(\"log_query_body\", False):\n            meta[\"query_body\"] = _extract_query_body(info)\n\n        if config.get(\"log_query_variables\", False):\n            meta[\"variables\"] = _extract_variables(info)\n\n        stash_meta_on_request(request, _REQUEST_ATTR, meta)\n\n    try:\n        return next(root, info, **kwargs)\n    except Exception as error:\n        # Record the error on the stashed metadata so the Django\n        # middleware can log it.\n        meta = getattr(request, _REQUEST_ATTR, None)\n        if meta is not None:\n            meta[\"error\"] = error\n        raise\n</code></pre>"},{"location":"dev/code_reference/package.html#nautobot_graphql_observability.metrics","title":"<code>nautobot_graphql_observability.metrics</code>","text":"<p>Prometheus metric definitions for GraphQL instrumentation.</p>"},{"location":"user/app_getting_started.html","title":"Getting Started with the App","text":"<p>This document provides a step-by-step tutorial on how to get the App going and how to use it.</p>"},{"location":"user/app_getting_started.html#install-the-app","title":"Install the App","text":"<p>To install the App, please follow the instructions detailed in the Installation Guide.</p>"},{"location":"user/app_getting_started.html#first-steps-with-the-app","title":"First Steps with the App","text":"<p>Once the app is installed and Nautobot has been restarted, metrics collection begins automatically. Here is how to verify it is working:</p>"},{"location":"user/app_getting_started.html#1-send-a-graphql-query","title":"1. Send a GraphQL Query","text":"<p>Use the Nautobot GraphQL API to run a query. You can use the GraphiQL interface at <code>/graphql/</code> or send a request via <code>curl</code>:</p> <pre><code>curl -X POST http://localhost:8080/api/graphql/ \\\n  -H \"Authorization: Token $NAUTOBOT_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"query GetDevices { devices { name } }\"}'\n</code></pre>"},{"location":"user/app_getting_started.html#2-check-the-metrics-endpoint","title":"2. Check the Metrics Endpoint","text":"<p>Browse to or query Nautobot's default Prometheus metrics endpoint:</p> <pre><code>curl http://localhost:8080/metrics/\n</code></pre> <p>You should see output similar to:</p> <pre><code># HELP graphql_requests_total Total number of GraphQL requests\n# TYPE graphql_requests_total counter\ngraphql_requests_total{operation_name=\"GetDevices\",operation_type=\"query\",status=\"success\"} 1.0\n\n# HELP graphql_request_duration_seconds Duration of GraphQL request execution in seconds\n# TYPE graphql_request_duration_seconds histogram\ngraphql_request_duration_seconds_bucket{le=\"0.01\",operation_name=\"GetDevices\",operation_type=\"query\"} 0.0\n...\n\n# HELP graphql_query_depth Depth of GraphQL queries\n# TYPE graphql_query_depth histogram\ngraphql_query_depth_bucket{le=\"1.0\",operation_name=\"GetDevices\"} 1.0\n...\n</code></pre>"},{"location":"user/app_getting_started.html#3-configure-prometheus-scraping","title":"3. Configure Prometheus Scraping","text":"<p>Add a scrape target in your <code>prometheus.yml</code>:</p> <pre><code>scrape_configs:\n  - job_name: \"nautobot-graphql\"\n    metrics_path: \"/metrics/\"\n    static_configs:\n      - targets: [\"nautobot:8080\"]\n</code></pre>"},{"location":"user/app_getting_started.html#4-enable-query-logging-optional","title":"4. Enable Query Logging (Optional)","text":"<p>To also log every GraphQL query, enable logging in your <code>nautobot_config.py</code>:</p> <pre><code>PLUGINS_CONFIG = {\n    \"nautobot_graphql_observability\": {\n        \"query_logging_enabled\": True,\n        \"log_query_body\": True,\n    }\n}\n</code></pre> <p>After restarting Nautobot, send a GraphQL query and check the Nautobot logs. With structlog JSON logging configured, each entry looks like:</p> <pre><code>{\n  \"event\": \"graphql_query\",\n  \"level\": \"info\",\n  \"logger\": \"nautobot_graphql_observability.graphql_query_log\",\n  \"timestamp\": \"2026-02-19T14:32:05.123456Z\",\n  \"operation_type\": \"query\",\n  \"operation_name\": \"GetDevices\",\n  \"user\": \"admin\",\n  \"duration_ms\": 42.3,\n  \"status\": \"success\",\n  \"query\": \"query GetDevices { devices { name } }\"\n}\n</code></pre>"},{"location":"user/app_getting_started.html#what-are-the-next-steps","title":"What are the next steps?","text":"<ul> <li>Review the App Configuration to tune which metrics and logging options are enabled.</li> <li>Set up Grafana dashboards using the provided templates.</li> <li>Route query logs to external systems \u2014 see Query Logging.</li> <li>Check out the Use Cases section for more examples.</li> </ul>"},{"location":"user/app_overview.html","title":"App Overview","text":"<p>This document provides an overview of the App including critical information and important considerations when applying it to your Nautobot environment.</p> <p>Note</p> <p>Throughout this documentation, the terms \"app\" and \"plugin\" will be used interchangeably.</p>"},{"location":"user/app_overview.html#description","title":"Description","text":"<p>The Nautobot App GraphQL Observability provides comprehensive observability for Nautobot's GraphQL API. It includes two Graphene middlewares that work together to provide both real-time metrics and structured query logging \u2014 without modifying Nautobot's core code.</p>"},{"location":"user/app_overview.html#prometheus-metrics","title":"Prometheus Metrics","text":"<p>The <code>PrometheusMiddleware</code> instruments every GraphQL query with Prometheus metrics, exposed at Nautobot's default <code>/metrics/</code> endpoint.</p>"},{"location":"user/app_overview.html#basic-metrics","title":"Basic Metrics","text":"Metric Type Labels Description <code>graphql_requests_total</code> Counter <code>operation_type</code>, <code>operation_name</code>, <code>status</code> Total number of GraphQL requests (success/error). <code>graphql_request_duration_seconds</code> Histogram <code>operation_type</code>, <code>operation_name</code> Duration of GraphQL request execution in seconds. <code>graphql_errors_total</code> Counter <code>operation_type</code>, <code>operation_name</code>, <code>error_type</code> Total number of GraphQL errors by exception type."},{"location":"user/app_overview.html#advanced-metrics","title":"Advanced Metrics","text":"Metric Type Labels Description <code>graphql_query_depth</code> Histogram <code>operation_name</code> Depth (nesting level) of GraphQL queries. <code>graphql_query_complexity</code> Histogram <code>operation_name</code> Complexity of GraphQL queries measured by total field count. <code>graphql_field_resolution_duration_seconds</code> Histogram <code>type_name</code>, <code>field_name</code> Duration of individual field resolution in seconds. <code>graphql_requests_by_user_total</code> Counter <code>user</code>, <code>operation_type</code>, <code>operation_name</code> Total number of GraphQL requests per authenticated user."},{"location":"user/app_overview.html#query-logging","title":"Query Logging","text":"<p>The <code>GraphQLQueryLoggingMiddleware</code> emits structured log entries for every GraphQL query using Python's <code>logging</code> module. Each log entry includes:</p> <ul> <li>Operation type (query/mutation)</li> <li>Operation name</li> <li>Authenticated user</li> <li>Duration in milliseconds</li> <li>Status (success/error)</li> <li>Error type (on failure)</li> <li>Query body (optional)</li> <li>Query variables (optional)</li> </ul> <p>Logs are emitted to the <code>nautobot_graphql_observability.graphql_query_log</code> logger and can be routed to any backend (file, syslog, ELK, etc.) via Django's <code>LOGGING</code> configuration.</p>"},{"location":"user/app_overview.html#audience-user-personas-who-should-use-this-app","title":"Audience (User Personas) - Who should use this App?","text":"<ul> <li>Nautobot Operators who need visibility into GraphQL API performance and usage patterns.</li> <li>SREs / Platform Engineers building observability stacks around Nautobot with Prometheus and Grafana.</li> <li>Network Automation Teams who want to monitor which GraphQL queries are slow, complex, or error-prone.</li> <li>Security Teams who want to track per-user API activity for auditing and forensics.</li> </ul>"},{"location":"user/app_overview.html#authors-and-maintainers","title":"Authors and Maintainers","text":"<ul> <li>Lydien SANDANASAMY (@slydien)</li> </ul>"},{"location":"user/app_overview.html#nautobot-features-used","title":"Nautobot Features Used","text":"<p>This app does not add models, views, or navigation items to Nautobot. It operates entirely at the Graphene middleware layer and provides:</p> <ul> <li>A Prometheus metrics middleware (<code>PrometheusMiddleware</code>) that instruments GraphQL resolvers with counters and histograms.</li> <li>A query logging middleware (<code>GraphQLQueryLoggingMiddleware</code>) that emits structured log entries for every GraphQL operation.</li> <li>An automatic monkey-patch of Nautobot's <code>GraphQLDRFAPIView</code> to load Graphene middleware from Django settings.</li> <li>Metrics are registered in the default Prometheus registry and automatically appear at Nautobot's default <code>/metrics/</code> endpoint.</li> </ul>"},{"location":"user/app_use_cases.html","title":"Using the App","text":"<p>This document describes common use-cases and scenarios for this App.</p>"},{"location":"user/app_use_cases.html#monitoring-query-performance","title":"Monitoring Query Performance","text":"<p>Use <code>graphql_request_duration_seconds</code> to identify slow GraphQL queries and track performance over time.</p> <p>Example PromQL to find the 95th percentile query duration:</p> <pre><code>histogram_quantile(0.95, rate(graphql_request_duration_seconds_bucket[5m]))\n</code></pre>"},{"location":"user/app_use_cases.html#tracking-per-user-activity","title":"Tracking Per-User Activity","text":"<p>When <code>track_per_user</code> is enabled (the default), the <code>graphql_requests_by_user_total</code> counter tracks which users are making GraphQL requests. This is useful for:</p> <ul> <li>Capacity planning: Identify heavy API consumers.</li> <li>Security auditing: Detect unusual query patterns from specific users.</li> <li>Troubleshooting: Correlate performance issues with specific user activity.</li> </ul> <p>Example PromQL to find the top 5 users by request count:</p> <pre><code>topk(5, sum by (user) (rate(graphql_requests_by_user_total[1h])))\n</code></pre>"},{"location":"user/app_use_cases.html#identifying-expensive-queries","title":"Identifying Expensive Queries","text":"<p>Use <code>graphql_query_depth</code> and <code>graphql_query_complexity</code> to detect queries that are deeply nested or request many fields:</p> <pre><code># Queries with depth &gt; 5\nhistogram_quantile(0.99, rate(graphql_query_depth_bucket[5m])) &gt; 5\n\n# Queries with complexity &gt; 100 fields\nhistogram_quantile(0.99, rate(graphql_query_complexity_bucket[5m])) &gt; 100\n</code></pre> <p>These metrics help you understand which queries may need optimization or which clients may need guidance on query best practices.</p>"},{"location":"user/app_use_cases.html#per-field-resolution-debugging","title":"Per-Field Resolution Debugging","text":"<p>When <code>track_field_resolution</code> is enabled, <code>graphql_field_resolution_duration_seconds</code> records the time spent resolving each individual field. This is useful for pinpointing slow resolvers during debugging.</p> <p>Warning</p> <p>Enabling <code>track_field_resolution</code> adds overhead to every field resolution. Use it for short-term debugging, not in production under heavy load.</p> <p>Example PromQL to find the slowest fields:</p> <pre><code>topk(10, sum by (type_name, field_name) (rate(graphql_field_resolution_duration_seconds_sum[5m])))\n</code></pre>"},{"location":"user/app_use_cases.html#alerting-on-error-rates","title":"Alerting on Error Rates","text":"<p>Use <code>graphql_errors_total</code> to set up alerts when GraphQL error rates spike:</p> <pre><code># Error rate as a percentage of total requests\nsum(rate(graphql_errors_total[5m])) / sum(rate(graphql_requests_total[5m])) * 100 &gt; 5\n</code></pre>"},{"location":"user/app_use_cases.html#monitoring-operation-types","title":"Monitoring Operation Types","text":"<p>Compare query vs mutation traffic to understand API usage patterns:</p> <pre><code>sum by (operation_type) (rate(graphql_requests_total[5m]))\n</code></pre>"},{"location":"user/app_use_cases.html#query-logging","title":"Query Logging","text":"<p>The app includes a separate logging middleware that emits structured log entries for every GraphQL operation. This complements the Prometheus metrics by providing per-request detail that can be searched, filtered, and forwarded to log aggregation systems.</p>"},{"location":"user/app_use_cases.html#enabling-query-logging","title":"Enabling Query Logging","text":"<p>Set <code>query_logging_enabled</code> to <code>True</code> in your <code>PLUGINS_CONFIG</code>:</p> <pre><code>PLUGINS_CONFIG = {\n    \"nautobot_graphql_observability\": {\n        \"query_logging_enabled\": True,\n        \"log_query_body\": True,\n        \"log_query_variables\": False,\n    }\n}\n</code></pre>"},{"location":"user/app_use_cases.html#log-output-format","title":"Log Output Format","text":"<p>The logging middleware emits structured records using Python's standard <code>logging</code> module under the logger name <code>nautobot_graphql_observability.graphql_query_log</code>. Each record carries the following fields as <code>LogRecord</code> attributes (via <code>extra</code>):</p> Field Type Description <code>event</code> <code>str</code> Always <code>\"graphql_query\"</code> <code>operation_type</code> <code>str</code> <code>\"query\"</code> or <code>\"mutation\"</code> <code>operation_name</code> <code>str</code> Named operation or comma-separated root fields for anonymous queries <code>user</code> <code>str</code> Authenticated username, or <code>\"anonymous\"</code> <code>duration_ms</code> <code>float</code> Total request duration in milliseconds <code>status</code> <code>str</code> <code>\"success\"</code> or <code>\"error\"</code> <code>error_type</code> <code>str</code> Exception class name \u2014 only present on error <code>query</code> <code>str</code> Full query text \u2014 only present when <code>log_query_body</code> is enabled <code>variables</code> <code>str</code> JSON-encoded variables \u2014 only present when <code>log_query_variables</code> is enabled"},{"location":"user/app_use_cases.html#structured-json-logging-with-structlog","title":"Structured JSON Logging with structlog","text":"<p>For production deployments sending logs to aggregation systems (Loki, Elasticsearch, Splunk, etc.), Nautobot's built-in <code>setup_structlog_logging</code> helper can be used to emit all log output as JSON. The query log fields will appear as top-level JSON keys.</p> <p>Add the following to your <code>nautobot_config.py</code>:</p> <pre><code>import structlog\nfrom nautobot.core.settings_funcs import setup_structlog_logging\n\n# Declare only the loggers that need explicit configuration:\n#   - django and nautobot are already defined in nautobot.core.settings.\n#   - django.request is not; list it so disable_existing_loggers=True doesn't silence it.\n#   - nautobot_graphql_observability.graphql_query_log must be listed directly because\n#     the logging middleware always sets propagate=False, so the record never reaches\n#     the root logger and needs a handler attached to this exact logger name.\nLOGGING = {\n    \"loggers\": {\n        \"django.request\": {\"level\": \"INFO\"},\n        \"nautobot_graphql_observability.graphql_query_log\": {\"level\": \"INFO\"},\n    },\n}\n\nsetup_structlog_logging(\n    LOGGING,\n    INSTALLED_APPS,\n    MIDDLEWARE,\n    log_level=\"INFO\",\n    plain_format=False,  # False \u2192 JSONRenderer; True \u2192 ConsoleRenderer (human-readable)\n)\n\n# setup_structlog_logging overwrites the formatter; append ExtraAdder afterwards so that\n# the extra fields (operation_type, operation_name, user, \u2026) are promoted from the\n# LogRecord into top-level JSON keys instead of being invisible in the event string.\n# In test mode setup_structlog_logging returns early without creating formatters, so guard.\nif \"formatters\" in LOGGING:\n    _fmt = LOGGING[\"formatters\"][\"default_formatter\"]\n    _fmt[\"foreign_pre_chain\"] = (*_fmt[\"foreign_pre_chain\"], structlog.stdlib.ExtraAdder())\n</code></pre> <p>Each query log entry will be emitted as a single JSON object:</p> <pre><code>{\n  \"event\": \"graphql_query\",\n  \"level\": \"info\",\n  \"logger\": \"nautobot_graphql_observability.graphql_query_log\",\n  \"timestamp\": \"2026-02-19T08:16:31.818110Z\",\n  \"operation_type\": \"query\",\n  \"operation_name\": \"GetDevices\",\n  \"user\": \"admin\",\n  \"duration_ms\": 162.4,\n  \"status\": \"success\",\n  \"query\": \"query GetDevices { devices { name } }\",\n  \"ip\": \"192.168.148.1\",\n  \"request_id\": \"0e2936fc-7989-4fcb-a63a-d0dd4d6bcea7\",\n  \"user_id\": null\n}\n</code></pre> <p>Note</p> <p><code>ip</code>, <code>request_id</code>, and <code>user_id</code> are injected automatically by <code>django_structlog.middlewares.RequestMiddleware</code>, which <code>setup_structlog_logging</code> adds to <code>MIDDLEWARE</code>.</p> <p>Note</p> <p><code>setup_structlog_logging</code> also configures the <code>nautobot</code> and <code>django</code> loggers (already present in Nautobot's base settings) with the same JSON formatter, so all Nautobot and Django log output is consistently structured.</p>"},{"location":"user/app_use_cases.html#routing-logs-to-external-systems","title":"Routing Logs to External Systems","text":"<p>If you are not using structlog, the logging middleware uses Python's standard <code>logging</code> module and can be routed to any backend via Django's <code>LOGGING</code> configuration:</p> <pre><code># nautobot_config.py\nLOGGING = {\n    \"version\": 1,\n    \"disable_existing_loggers\": False,\n    \"handlers\": {\n        \"graphql_file\": {\n            \"level\": \"INFO\",\n            \"class\": \"logging.FileHandler\",\n            \"filename\": \"/var/log/nautobot/graphql_queries.log\",\n        },\n    },\n    \"loggers\": {\n        \"nautobot_graphql_observability.graphql_query_log\": {\n            \"handlers\": [\"graphql_file\"],\n            \"level\": \"INFO\",\n            \"propagate\": False,\n        },\n    },\n}\n</code></pre>"},{"location":"user/app_use_cases.html#security-considerations","title":"Security Considerations","text":"<p>Warning</p> <p>Enabling <code>log_query_body</code> will log the full GraphQL query text, and <code>log_query_variables</code> will log query variables which may contain sensitive data (passwords, tokens, etc.). Only enable these in environments where log access is properly secured.</p>"},{"location":"user/external_interactions.html","title":"External Interactions","text":"<p>This document describes external dependencies and prerequisites for this App to operate.</p>"},{"location":"user/external_interactions.html#external-system-integrations","title":"External System Integrations","text":""},{"location":"user/external_interactions.html#from-other-systems-to-the-app","title":"From Other Systems to the App","text":""},{"location":"user/external_interactions.html#prometheus-scraping","title":"Prometheus Scraping","text":"<p>The app registers its metrics in the default <code>prometheus_client</code> registry. They are automatically included in Nautobot's default <code>/metrics/</code> endpoint (served by <code>PrometheusMetricsMiddleware</code>).</p> <ul> <li>URL: <code>/metrics/</code></li> <li>Format: Standard Prometheus text exposition format</li> <li>Authentication: None required (bypasses DRF)</li> </ul> <p>Add this to your Prometheus configuration:</p> <pre><code>scrape_configs:\n  - job_name: \"nautobot-graphql\"\n    metrics_path: \"/metrics/\"\n    scrape_interval: 15s\n    static_configs:\n      - targets: [\"nautobot-host:8080\"]\n</code></pre>"},{"location":"user/external_interactions.html#grafana-dashboards","title":"Grafana Dashboards","text":"<p>The repository includes pre-built Grafana dashboard templates in the <code>docs/grafana/dashboards/</code> directory:</p> Dashboard File Description GraphQL Performance <code>graphql-performance.json</code> Query duration percentiles, request rates, error rates, depth and complexity distributions. Nautobot HTTP Overview <code>nautobot-http-overview.json</code> HTTP-level metrics for the Nautobot instance. System Health <code>system-health.json</code> Process-level metrics (CPU, memory, open file descriptors). Database and Cache <code>database-and-cache.json</code> Database connection pool and cache hit/miss metrics. <p>To import a dashboard:</p> <ol> <li>Open Grafana and navigate to Dashboards &gt; Import.</li> <li>Upload the JSON file or paste its contents.</li> <li>Select your Prometheus data source.</li> <li>Save the dashboard.</li> </ol>"},{"location":"user/external_interactions.html#alerting-rules","title":"Alerting Rules","text":"<p>An example Alertmanager rule file is provided at <code>docs/grafana/alerts/nautobot-alert-rules.yaml</code>. It includes rules for:</p> <ul> <li>High GraphQL error rates</li> <li>Slow query duration thresholds</li> <li>High query depth/complexity</li> </ul> <p>Import these rules into your Prometheus or Grafana alerting configuration.</p>"},{"location":"user/external_interactions.html#query-log-integration","title":"Query Log Integration","text":"<p>The logging middleware emits structured log entries to the <code>nautobot_graphql_observability.graphql_query_log</code> Python logger. These logs can be forwarded to external systems via Django's <code>LOGGING</code> configuration:</p> Target Handler Class Notes File <code>logging.FileHandler</code> Write to a dedicated log file for rotation and archival. Syslog <code>logging.handlers.SysLogHandler</code> Forward to a centralized syslog server. HTTP <code>logging.handlers.HTTPHandler</code> Send log entries to an HTTP endpoint (e.g., Logstash, Splunk HEC). Console <code>logging.StreamHandler</code> Default behavior \u2014 writes to stderr. <p>See Query Logging for configuration examples.</p>"},{"location":"user/external_interactions.html#nautobot-rest-api-endpoints","title":"Nautobot REST API Endpoints","text":"<p>This app does not add any REST API endpoints. All metrics are available at Nautobot's default <code>/metrics/</code> endpoint.</p>"},{"location":"user/faq.html","title":"Frequently Asked Questions","text":""},{"location":"user/faq.html#why-dont-i-see-any-metrics-after-installing-the-app","title":"Why don't I see any metrics after installing the app?","text":"<p>Metrics are only recorded when GraphQL queries are executed against the <code>/api/graphql/</code> endpoint. Send a test query and then check the metrics at Nautobot's default <code>/metrics/</code> endpoint.</p> <p>If metrics still don't appear, verify that:</p> <ol> <li>The app is listed in <code>PLUGINS</code> in your <code>nautobot_config.py</code>.</li> <li>Nautobot was restarted after installation.</li> <li>The <code>graphql_metrics_enabled</code> setting is <code>True</code> (the default).</li> </ol>"},{"location":"user/faq.html#how-do-i-configure-prometheus-to-scrape-this-endpoint","title":"How do I configure Prometheus to scrape this endpoint?","text":"<p>Add the following to your <code>prometheus.yml</code>:</p> <pre><code>scrape_configs:\n  - job_name: \"nautobot-graphql\"\n    metrics_path: \"/metrics/\"\n    static_configs:\n      - targets: [\"nautobot-host:8080\"]\n</code></pre> <p>No authentication is required \u2014 Nautobot's <code>/metrics/</code> endpoint bypasses DRF.</p>"},{"location":"user/faq.html#what-is-the-performance-impact-of-this-app","title":"What is the performance impact of this app?","text":"<p>The basic metrics (request count, duration, errors) add negligible overhead since they only instrument the root resolver.</p> <p>Enabling <code>track_query_depth</code> and <code>track_query_complexity</code> adds a small amount of overhead to parse the query AST after resolution. This is typically sub-millisecond.</p> <p>Enabling <code>track_field_resolution</code> instruments every field resolver in every query. This can add measurable overhead for complex queries with hundreds of fields. It is recommended to leave this disabled in production and only enable it for short-term debugging.</p>"},{"location":"user/faq.html#how-does-this-work-with-multiple-nautobot-worker-processes","title":"How does this work with multiple Nautobot worker processes?","text":"<p>Set the <code>PROMETHEUS_MULTIPROC_DIR</code> environment variable to a writable directory before starting Nautobot:</p> <pre><code>export PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus_multiproc\nmkdir -p \"$PROMETHEUS_MULTIPROC_DIR\"\n</code></pre> <p>The <code>prometheus_client</code> library will use shared files in this directory to aggregate metrics across all worker processes. Nautobot's <code>/metrics/</code> endpoint automatically handles multiprocess aggregation.</p>"},{"location":"user/faq.html#why-does-the-app-monkey-patch-graphqldrfapiview","title":"Why does the app monkey-patch GraphQLDRFAPIView?","text":"<p>Nautobot 3.x's <code>GraphQLDRFAPIView.init_graphql()</code> has a bug: when <code>self.middleware</code> is <code>None</code> (the default), it does not load middleware from the <code>GRAPHENE[\"MIDDLEWARE\"]</code> Django setting. The app patches this method during <code>AppConfig.ready()</code> to ensure configured Graphene middleware is properly loaded.</p>"},{"location":"user/faq.html#how-do-i-enable-graphql-query-logging","title":"How do I enable GraphQL query logging?","text":"<p>Set <code>query_logging_enabled</code> to <code>True</code> in your <code>PLUGINS_CONFIG</code>:</p> <pre><code>PLUGINS_CONFIG = {\n    \"nautobot_graphql_observability\": {\n        \"query_logging_enabled\": True,\n    }\n}\n</code></pre> <p>Optionally enable <code>log_query_body</code> and <code>log_query_variables</code> to include the query text and variables in each log entry. See Query Logging for details on routing logs to external systems.</p>"},{"location":"user/faq.html#why-arent-my-query-logs-appearing","title":"Why aren't my query logs appearing?","text":"<p>The logging middleware uses a dedicated logger (<code>nautobot_graphql_observability.graphql_query_log</code>) that writes to stderr by default. If you have a custom Django <code>LOGGING</code> configuration that suppresses loggers not explicitly listed, you may need to add an entry for this logger. See Routing Logs to External Systems.</p>"},{"location":"user/faq.html#can-i-use-metrics-and-logging-independently","title":"Can I use metrics and logging independently?","text":"<p>Yes. The two middlewares are independent:</p> <ul> <li>Set <code>graphql_metrics_enabled: True</code> and <code>query_logging_enabled: False</code> for metrics only.</li> <li>Set <code>graphql_metrics_enabled: False</code> and <code>query_logging_enabled: True</code> for logging only.</li> <li>Enable both for full observability.</li> </ul>"},{"location":"user/faq.html#can-i-use-this-app-without-nautobot","title":"Can I use this app without Nautobot?","text":"<p>The <code>PrometheusMiddleware</code> and <code>GraphQLQueryLoggingMiddleware</code> classes are standard Graphene middlewares. While this Nautobot app handles the automatic setup and configuration, the middlewares themselves could be used in any Graphene-based project by manually adding them to your <code>GRAPHENE[\"MIDDLEWARE\"]</code> setting.</p>"}]}